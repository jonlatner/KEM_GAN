\documentclass[a4paper,11pt]{style/uneceart}

\usepackage{style/UNECE2025}
\usepackage{enumerate}
\usepackage[T1]{fontenc}      % proper font encoding
\usepackage[table]{xcolor}

\definecolor{unece_color}{RGB}{84, 141, 212}


\usepackage{booktabs}

\usepackage{rotating} % Rotating table
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{array} % necessary for word wrap within a table


%%% ----------------------------------------------------------------------
%%% ---   please fill-in your personal data:   ---------------------------

%% the title of your contribution in capital letters:
\newcommand{\TITLE}{\textbf{Buyer Beware: Understanding the trade-off between utility and risk in CART based models using simulation data} \\}

%% author:
\newcommand{\AUTHOR}{Jonathan Latner$^{1}$, Marcel Neunhoeffer$^{1,2}$, J\"{o}rg Drechsler$^{1,3}$}

%% your organisation
\newcommand{\ORGANISATION}{\vspace{2mm} \\  $^{1}$Institute for Employment Research, Nuremberg, Germany \\ 
$^{2}$Ludwig-Maximilians-Universit\"at, Munich, Germany \\ 
$^{3}$University of Maryland, College Park, USA
}
\newcommand{\EMAIL}{jonathan.latner@iab.de}

%% abstract
\newcommand{\ABSTRACT}{This paper evaluates identity risk and attribute risk in synthetic data generated by CART-based models, using both a controlled simulated dataset and publicly available data. We find that common privacy metrics may fail to detect disclosure risks and, in some cases, misrepresent actual privacy threats. Additionally, CART-based models, while maintaining high statistical utility, may compromise privacy protection under strong assumptions regarding the attacker knowledge. Our findings highlight challenges in measuring privacy risk in synthetic data and suggest improvements for more accurate risk assessment.}


\begin{document}
%% Cover page with title and abstract
\input{UNECE2025cover.tex}

\title{Buyer Beware: Understanding the trade-off between utility and risk in CART based models using simulation data}

\section{Introduction}

In this paper, we ask whether commonly used privacy measures accurately capture disclosure risk in synthetic data generated by CART-based models  The generation of synthetic data has gained prominence as a means to share data while preserving privacy. There are numerous methods and tools, both for generating synthetic data and measuring privacy.  On the one hand, there is a general perception that generating synthetic data are easy \cite{latner2024generating}, and to a certain degree this is true.  On the other hand, it is not always clear if the resulting synthetic data are in fact providing privacy protection, especially if no formal privacy methods are used.

We evaluate standard and easily available privacy measures to estimate risk from synthetic data generated by CART based models.  We use CART because previous research indicates that CART-based synthetic data generators (SDGs) generate synthetic data with both high statistical utility and relatively low privacy risks compared to other methods \cite{little2025synthetic,fossing2024evaluation,dankar2021fake}.  In addition, we use two measures of risk: identity risk and attribute risk \cite{raab2025practicalprivacymetricssynthetic}.  While some argue that privacy is a property of the algorithm that created the synthetic data, not the synthetic data itself \cite{jordon2022synthetic}, these measures reflect the current state of the art in evaluation of statistical disclosure control in synthetic data \cite{raab2025practicalprivacymetricssynthetic}.  Risk evaluations are also recommended for synthetic data generated using differential privacy methods (XXXX).  Therefore, our results have broader relevance, contributing to our understanding of both the evaluation of privacy risk measures and the assessment of synthetic data generators.

We use both simulated and real survey data for our analysis. The simulated data repliacte a design by Reiter \cite{reiter2014bayesian}, consisting of 1.000 observations and four binary variables. Crucially, the last or 1.000$^{th}$ observation is a unique combination of the four binary variables.  In so doing, we create data with an observation we want to protect using synthetic data generated from a CART model.  We also use publicly available data from Social Diagnosis 2011 (SD2011) to extend our analysis beyond a simulated setting and into a real-world setting.

Our approach follows a structured framework. Using the simulated data and drawing on concepts from the attack model in differential privacy, we estimate risk measures for an attacker with full knowledge of the synthetic data generator, all synthetic observations, and all original observations except the disclosive record.  First, we test whether an attacker can identify the disclosive record.  They can.  Next, we test whether the privacy measures detect a known disclosure risk in the simulated data. They do not.  Finally, we extend the analysis using the SD2011 data to assess whether the measurement problem is specific to the simulated case or part of a broader challenge in detecting privacy risk. The results suggest the latter.

Conditional on the assumption of a strong attacker model, we make three contributions.  Common risk measures both may not capture disclosure risk in synthetic data generated from simulated data, and also may misstate privacy risks in real data.  Further, relatedly, and in contrast to previous research, CART-based models may produce synthetic data that sacrifices privacy protection for statistical utility.  Finally, we propose some solutions for measuring disclosure risk.  More generally, users interested in measuring privacy risk should be aware of the challenges we describe here.

\section{Risk measures}

The literature on privacy measures for synthetic data is well-developed \cite{wagner2018technical}.  One reason why there are so many measures of privacy is because there is no one agreed upon understanding of either what defines risk nor how one should measure it.  We use two commonly understood measures of privacy implemented by the Synthpop package in R (version 1.9-0) \cite{raab2025practicalprivacymetricssynthetic}: identity risk and attribute risk.  

\subsubsection{Identity risk} measures the ability to identify individuals in the data from a set of known characteristics, i.e. `keys'. The maximum number of keys are one less than the total number of variables in the data.  

The following steps are used to calculate identity risk.  For a given set of keys ($q$), the intruder will look for the unique records in the original data ($UiO$) that are also unique in the synthetic data ($UiS$).  $repU$ (replicated uniques) is the measure of identity risk and defined by equation \ref{eq:repU}:

\begin{equation}
repU = 100 \sum (s_{.q}|d_{.q} = 1 \land s_{.q} = 1 )/N_{d}
\label{eq:repU}
\end{equation}

where $d_{.q}$ is the count of records in the original data with the keys corresponding to a given value of $q$ and $s_{.q}$ is the equivalent count for the synthetic data.  In a given value of $q$, $s_{.q}|d_{.q} = 1$ is a unique record in the original data conditional on also existing in the synthetic data.  $s_{.q} = 1$ is the unique record in the synthetic data.  This is summed over all unique values of $q$ and divided by the total number of records in the data ($N_{d}$) and multiplied by 100 to transform the ratio into a percentage.

\subsubsection{Attribute risk} measures the ability to identify a previously unknown characteristic of an individual.  In this approach, an attacker has access to synthetic data, knows one or more identifiers in the original data (i.e. $q$ or keys, as above), and wants to infer a sensitive attribute ($t$).  Attribute risk or $DiSCO$ (Disclosive in Synthetic Correct in Original) is the subset of records in the original data for which the keys ($q$) in the synthetic data are disclosive. $q$ is disclosive if all records in the synthetic data with the same $q$ have a constant target ($t$), i.e. no variation in $t$, as defined by the following equation \ref{eq:DiSCO}:

\begin{equation}
DiSCO = 100 \sum^{q} \sum^{t} (d_{tq} | ps_{tq} = 1) / N_{d}
\label{eq:DiSCO}
\end{equation}

where $ps_{tq} = s_{tq}/s_{.q}$ are the column proportions, and $d_{tq} | ps_{tq} = 1$ indicates whether the synthetic data matches the original data for the combination of $t$ and $q$ given the condition that the synthetic data for the combination of $t$ and $q$ is disclosive (i.e., target $t$ is uniquely determined by the keys $q$).  This is summed over unique values of $t$ and unique values of $q$ and divided by the total number of records in the data ($N_{d}$) and multiplied by 100 to transform the count into a percentage.

\section{Data}

To examine our question of interest, we use two types of data: simulated and real data.  Following Reiter et al. \cite{reiter2014bayesian}, we simulate one data set with 1.000 observations and four binary categorical variables.  This is our `original' data set.  The first 999 records are sampled from a multinomial distribution for all combinations of var1(0,1), var2(0,1), var3(0,1), var4(0,1), except the last 1000$^{th}$ record is a unique combination (var1 = 1, var2 = 1, var3 = 1, var4 = 1).  Next, we generate one synthetic data set from a CART-based SDG using the Synthpop package in R with default parameters (seed=1237).  As a sensitivity test, we create 10 synthetic data sets from the original data.  The value of the simulated data is that we know there is a disclosive record because we created it.

At the same time, an obvious critique is that the simulated data set is not representative of real world data.  We agree with this critique, even though the value of the simulated data is to illustrate a sort of `bound' on disclosure risks.  Therefore, we use a second data set from Social Diagnosis 2011 (SD2011), publicly available from the Synthpop package in R to examine our question of interest using real world data.

\section{Results from simulated data}

We begin with the simulated data that contains four categorical variables and 1.000 observations, one of which is unique or disclosive. For both the original and synthetic data, figure \ref{fig:frequency_compare} shows the frequency distribution within each of the four variables and figure \ref{fig:histogram_compare} the frequency histogram across all four variables.  In the original data, there is one observation with a combination (1,1,1,1) that is not visible if we look at the distribution within each of the variables.  

\begin{figure}[!h]
    \centering
    \caption{Compare original and synthetic data}
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{../graphs/graph_cart_frequency_compare.pdf}
        \caption{Frequency}
        \label{fig:frequency_compare}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{../graphs/graph_cart_histogram_compare.pdf}
        \caption{Histogram}
        \label{fig:histogram_compare}
    \end{subfigure}
    \label{fig:compare}
\end{figure}

On the one hand, the synthetic data generated by the CART model have high levels of utility because they almost perfectly match the frequency of values in the original data not only within the four variables but also across all four variables.  On the other hand, the synthetic data perfectly replicates the single disclosive record.

The disclosive record does not disappear if we generate multiple synthetic data copies (10), as shown in figure \ref{fig:cart_histogram_compare_10}.  The frequency of the disclosive record ranges from 0 (2 data sets), 1 (5 data sets), 2 (1 data sets), and 3 (2 data sets).\footnote{For reference, the frequency of the disclosive record would be similar if we created 100 synthetic data sets, ranging from 0 (41 data sets), 1 (38 data sets), 2 (14 data sets), and 3 (7 data sets).}  Regardless of whether one, five, or ten synthetic data sets were released, it would be clear which record was the disclosive record.  

\begin{figure}[!h]
    \centering
    \caption{Frequency}
    \includegraphics[width=\textwidth]{../graphs/graph_cart_histogram_compare_10_v1.pdf}
    \label{fig:cart_histogram_compare_10}
\end{figure}


\subsection{The attack}

One way to think about disclosure risk is to imagine a game between two entities.  On one side, there is a statistical agency who has the data and wants to release it in a privacy preserving way.  On the other side, there is an attacker who wants to identify someone in the data (either membership or attribute inference). The question is what can the attacker learn from a released synthetic data set about an individual they do not have knowledge of?

In this scenario, we assume a strong attacker, similar to the attack model used in differential privacy (DP). The attacker is assumed to know the exact SDG used to generate the synthetic data, which in our case is sequential CART, and to have access to all records in the original data except the final one. Moreover, given the small domain of the variables, the attacker knows that the last record must be one of the 16 possible combinations of the four binary variables. In this attack, the attacker sees the released synthetic dataset and generates synthetic data using the same sequential CART procedure for each of the 16 possible true values of the last record. By comparing the resulting synthetic datasets with the released one, the attacker updates their posterior beliefs about which of the 16 candidate records is most likely the true one.

Figure \ref{fig:attacker_default} illustrates the results of the attack.  In the top left cell, the attacker guesses that the last record in the original data is 0,0,0,0.  They then generate 10 synthetic data sets using a CART-based SDG and compare the histogram to the released synthetic data, as shown in figure \ref{fig:cart_histogram_compare_10}.  

\begin{figure}[!h]
    \centering
    \caption{Histogram of 16 worlds x 10 synthetic datasets}
    \resizebox{\textwidth}{!}{\includegraphics{../graphs/graph_attacker_default_v1.pdf}}
    \label{fig:attacker_default}
\end{figure}

If the attacker guesses that the last record is 0,0,0,0, then they are not able to replicate the single unique record in the synthetic data.  Next, they update their beliefs about the last record and guess that the last record in the original data is 0,0,0,1.  They then repeat the process as described above.  This is shown in the top row, second column from the left.  Like their first guess, they cannot replicate the released synthetic data.  

The attacker then repeats this process for all 16 possible combinations of the last record.  Finally, if they guess that the last record is 1,1,1,1, then they are able to replicate the released synthetic data, as shown in the bottom, right cell.  The result is a successful attack with confirmation that the guess about the values of the last, unique observation is correct.

\subsection{Measuring privacy risk}

The larger concern is whether we are able to measure this disclosure.  Results from our simulated data show that CART produces synthetic data that is disclosive because it replicates unique records from the original data set without adding sufficient noise.  Other research also indicates that CART-based SDGs can produce synthetic data with high levels of utility because they reproduce a high proportion of the original data \cite{manrique2018bayesian}.  The question is can we measure this observable risk?

In table \ref{table:disclosure_risk_1}, the columns display identity ($repU$ - equation \ref{eq:repU}) and attribute risk ($DiSCO$ - equation \ref{eq:DiSCO}) measures (the columns) in the original and synthetic data (the rows).  For reference, we replicated table \ref{table:disclosure_risk_1} with 10 synthetic copies from figure \ref{fig:cart_histogram_compare_10}, as shown in table \ref{table:disclosure_risk_10} in the Appendix.  Results are qualitatively similar.  

\begin{table}[]
    \centering
    \caption{Disclosure risk measures}
    \input{../tables/table_disclosure_risk_1.tex}
    \label{table:disclosure_risk_1}
\end{table}

The identity risk measures are zero for both the original and synthetic data. In this example, the first three binary variables ($q$) are treated as keys, although this choice is arbitrary since all variables are binary. These three attributes yield $2^3 = 8$ possible combinations, none of which are unique in the dataset. Consequently, under the $repU$ measure, the risk of identity disclosure is zero because no record can be uniquely identified based on the chosen keys.

However, the attribute risk measures are also 0 for both the original and synthetic data.  Here, the target is the 4th binary variable.  This is a problem because we know that when $q=111$, there is a unique record if $t=1$.  

How can the metric report no attribute disclosure risk when we know that such a risk exists in reality? The explanation lies in how the metric is defined: it registers an attribute disclosure risk only when $t$ is constant, meaning there is no variation in $t$ within the set of records sharing the same $q$. Under this definition, the metric reports a risk only when there are zero copies of the unique record in the synthetic data. In other cases, the real possibility of attribute disclosure may still be present, but it is not captured by the metric.


We can see this issue more clearly if we examine the frequency table from 10 synthetic data copies, as shown in figure \ref{fig:cart_histogram_compare_10}.  The underlying data and resulting privacy measures are shown in the appendix (table \ref{table:frequency_10_data_sets} and \ref{table:disclosure_risk_10}).  If there is at least 1 unique record in the synthetic data, then there is no attribute risk because there are 2 values of $t$ within $q$.  At the same time, if a synthetic data set is released without the unique record, then there is an attribute disclosure risk because there is only 1 value of $t$ within $q$.  

Therefore, the attribute disclosure risk measure indicates a disclosure risk when we know there is no disclosure risk, and indicates there is no disclosure risk when we know there is a disclosure risk.  The exercise illustrates a flaw in the measure of attribute disclosure risk using our simulated data.  We know there is a disclosure problem (because we created it), but the problem is not identified by the measure.  

\section{A real world example}

In the papers describing privacy measures implemented in Synthpop \cite{raab2024privacy,raab2025practicalprivacymetricssynthetic}, the author(s) generate 5 synthetic data sets to illustrate their method for measuring attribute disclosure by identifying values in the target variable \texttt{depress} from keys: \texttt{sex} \texttt{age} \texttt{region} \texttt{placesize}.  As described above, their preferred measure of attribute disclosure risk (DiSCO) is the set of records in the synthetic data with a constant target ($t$) for a set of keys ($q$).  In other words, there is no variation of $t$ within $q$.  In their example, attribute risk in the original data is 53\%, which is reduced to about about 9\% in the synthetic data (as shown in the table \ref{tab:attribute_risk_sd2011} appendix).  

To illustrate why it is a problem to measure attribute disclosure as the set of records with constant $t$ within $q$, we set $t$ as constant for all observations in all 5 synthetic data sets.  0 was chosen because it is the most frequent value in the variable \texttt{depress} (22\% of all records).  By definition, this reduces attribute disclosure risk.  However, according to the measure of attribute disclosure risk used by the package, the risk increased to around 15\%.

We note that this problem is already understood and described by the package authors \cite{raab2025practicalprivacymetricssynthetic}.  They provide a parameter where a user may check for a target where a high proportion of records have one level (\texttt{check\_1way}).  However, the problem is not the package, the problem is the definition.

The exercise illustrates a flaw in the measure of attribute disclosure risk using real world data.  We modified the synthetic data by setting a uniform value for the target variable: \texttt{depress} = 0.  If the measure correctly measured risk, then the risk measure should decline, but it rises.  Therefore, the attribute risk measure indicates that disclosure risk increased when we know disclosure risk decreased (because we decreased it).  

\section{Conclusion}

In this study, we use two data sets, one simulated data with a single disclosive record and one real-world data, to demonstrate three ideas.  First, not only do common privacy metrics not detect the disclosure risks we know exist in the simulated data, but they can also misstate the disclosure risk in real-world data.  Second, relatedly, CART-based synthetic data generators with default parameters create synthetic data with high levels of utility that reproduce the original data without protection for the dislosive record.  Therefore, CART-based models are not inherently immune to the utility-privacy trade-off.  Finally, it is possible to increase protection by adding noise to the synthetic data with simple adjustments to the default parameters, but the cost is to reduce utility.  The question is why one would reduce utility if there is no indication there was a disclosure problem?  Given these results, it is important for users interested in reducing disclosure risk to better understand not only how SDGs generate synthetic data, but also how common privacy measures work.  There is no one size fits all solution. 

\subsubsection{Acknowledgements} This work was supported by a grant from the German Federal Ministry of Research, Technology, and Space (BMFTR grant number 16KISA096) with funding from the European Union-NextGenerationEU.  Reproducible files are located here: \url{https://github.com/jonlatner/KEM\_GAN/tree/main/latner/projects/simulation}

\subsubsection{Disclosure of Interest} The authors have no competing interests to declare that are relevant to the content of this article.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{chicago}
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\appendix

\section{}\label{appendix}
\setcounter{figure}{0}    
\setcounter{table}{0}    
\renewcommand*\thetable{\Alph{section}.\arabic{table}}
\renewcommand*\thefigure{\Alph{section}.\arabic{figure}}
\renewcommand{\theHfigure}{\Alph{section}.\arabic{table}}
\renewcommand{\theHtable}{\Alph{section}.\arabic{figure}}

\begin{table}[!ht]
    \centering
    \caption{Frequency statistics for original and 10 synthetic data sets}
    \rowcolors{1}{white}{lightgray}
    \input{../tables/table_frequency.tex}
    \label{table:frequency_10_data_sets}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{Risk measures from 10 synthetic data sets}
    \rowcolors{1}{white}{lightgray}
    \input{../tables/table_disclosure_risk_10.tex}
    \label{table:disclosure_risk_10}
\end{table}


\begin{figure}[!h]
    \centering
    \caption{Compare original and synthetic data with different hyperparameters}
    \begin{subfigure}{0.9\textwidth}
        \includegraphics[width=\textwidth]{../graphs/graph_cart_modified_mb_histogram_compare_10_v2.pdf}
        \caption{Minimum bucket (default is 5)}
        \label{fig:attacker_modified_mb_sensitivity}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.9\textwidth}
        \includegraphics[width=\textwidth]{../graphs/graph_cart_modified_cp_histogram_compare_10_v2.pdf}
        \caption{Complexity parameter (default is 10$^{-8}$)}
        \label{fig:attacker_modified_cp_sensitivity}
    \end{subfigure}
    \label{fig:compare_modified_sensitivity}
\end{figure}


\begin{table}[]
    \centering
    \caption{Risk measures for \texttt{depress} from keys: \texttt{sex}, \texttt{age}, \texttt{region}, \texttt{placesize} (SD2011)}
    % \rowcolors{1}{white}{lightgray}
    \input{../tables/table_disclosure_risk_sd2011.tex}
    \label{tab:attribute_risk_sd2011}
\end{table}


\end{document}
