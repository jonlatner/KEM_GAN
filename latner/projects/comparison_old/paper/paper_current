\documentclass[runningheads]{llncs}
\usepackage[T1]{fontenc}

\usepackage[table]{xcolor}

\usepackage{booktabs}
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage[pdfstartview=XYZ,
bookmarks=true,
colorlinks=true,
linkcolor=blue,
urlcolor=blue,
citecolor=blue,
pdftex,
bookmarks=true,
linktocpage=true, % makes the page number as hyperlink in table of content
hyperindex=true
]{hyperref}

\usepackage{orcidlink} % orcidlink
\usepackage{marvosym} %letter symbol
\usepackage{rotating} % Rotating table
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath}

\newcommand{\jl}[1]{\scriptsize {\bf \color{violet}[JL: #1]}\normalsize}
\newcommand{\jd}[1]{\scriptsize {\bf \color{red}[JD: #1]}\normalsize}
\newcommand{\mn}[1]{\scriptsize {\bf \color{blue}[MN: #1]}\normalsize}

\title{Generating synthetic data is complicated: Know your data and know your generator}
\titlerunning{Generating synthetic data is complicated}

\author{Jonathan Latner\inst{1 (\text{\Letter})\orcidlink{0000-0002-1825-0097}} \and
Marcel Neunhoeffer\inst{1,2 \orcidlink{0000-0002-9137-5785}}  \and
J\"{o}rg Drechsler\inst{1,2,3 \orcidlink{0009-0009-5790-3394}}}

\authorrunning{Latner et al., \the\year}

%\author[1,2]{Marcel Neunhoeffer}
%\author[1]{Jonathan Latner}
%\author[1, 2, 3]{J\"{o}rg Drechsler}
%\affil[1]{Institute for Employment Research, Nuremberg, Germany}
%\affil[2]{Ludwig-Maximilians-Universit\"at, Munich, Germany}
%\affil[3]{University of Maryland, College Park, USA}

\institute{Institute for Employment Research, Nuremberg, Germany 
\email{\{jonathan.latner, marcel.neunhoeffer,joerg.drechsler\}@iab.de} \and
Ludwig-Maximilians-Universit\"at, Munich, Germany \and
University of Maryland, College Park, USA
}


\begin{document}

\maketitle 

\begin{abstract}

In recent years, more and more synthetic data generators (SDG) based on various modeling strategies have been implemented as Python libraries or R packages. With this proliferation of ready-made SDGs comes a widely held perception that generating synthetic data is easy.  We show that generating synthetic data is a complicated process that requires one to understand both the original dataset as well as the synthetic data generator (SDG).  We make two contributions to the literature in this topic area.  First, we show that it is just as important to pre-process or clean the data as it is to tune the SDG in order to create synthetic data with high levels of utility. Second, we illustrate that it is critical to understand the methodological details of the SDG to be aware of potential pitfalls and to be aware for which types of analysis tasks one can expect high levels of analytical validity.

%I dropped reference to confidentiality and privacy because this paper does not address those issues
\keywords{Data utility \and synthetic data generator \and CTGAN \and DataSynthesizer \and synthpop}
\end{abstract}

\section{Introduction}
The idea of synthetic microdata\footnote{Throughout the paper, when we refer to data, we are referring to classical microdata (i.e., one observation per individual unit), as opposed to releases of summary tables.} for statistical disclosure limitation has been introduced more than 30 years ago \cite{rubin1993statistical,little1993statistical,liew1985data} and has become increasingly popular in recent years \cite{jordon2022synthetic,drechsler202330}. The general appeal of synthetic data is obvious: synthetic data promises to mimic the statistical properties of the original data while maintaining the privacy of individual records. In practice, releasing synthetic data means fitting a model to the original data and then generating new data based on this model.  The new interest in synthetic data was spurred by the development of new methods and algorithms to generate synthetic data, many of which are available as open-source software \cite[see, e.g., ][]{nowok2016synthpop,ping2017datasynthesizer,ctgan}. At the same time, private companies offer synthetic data generation to share data as a service for companies and public administration.

This leaves the general impression that generating synthetic data has become easy and that software libraries offer a one-size-fits-all solution. While it is indeed easy (even for novices) to generate synthetic data, it is hard to generate high-quality data. This paper addresses the question: What makes generating high-quality synthetic data hard? In particular, we focus on the data one wants to synthesize and the synthetic data generators (SDG) one wishes to use. We show that one must possess good knowledge of the data and the generator when deciding which SDG is appropriate for what data. 

\section{Study design}\label{sec:study_design}

In this study, we illustrate the problems in generating high quality synthetic data with limited knowledge of the data and/or the generator.  To do so, we use three different SDGs and one dataset. The SDGs we use (CTGAN, DataSynthesizer, and synthpop) are commonly compared and contrasted in the literature \cite{dankar2021fake,little2022comparing}.  We use two utility measures (pMSE and computational run-time, as defined in Appendix \ref{appendix:utility_measures}) as well as one-way frequency tables to graph the distribution of variables between synthetic and original data.  For each SDG and respective parameter setting (as we discuss below), five fully synthetic datasets were generated, each using a different random seed.

\subsection{Data}

The data we use are called, Social Diagnosis 2011 - Objective and Subjective Quality of Life in Poland (SD2011) and are included as part of the synthpop package, but are also publicly available.\footnote{ \url{http://www.diagnoza.com/index-en.html}}  There are 35 variables and 5.000 observations, 21 variables are categorical (or factor), and 14 variables are numeric.  

There are two main reasons why we use SD2011 data.  One is data dimensionality, which refers to rows, columns, and unique values within categorical variables.  Data dimensionality affects the duration in time required to generate the synthetic data.  While we do not consider SD2011 to be a high dimensional dataset, it is higher dimensional than data used in previous evaluations \cite{dankar2021fake,little2022comparing}.  While previous research used data with more columns, the data contained fewer rows or if data contained more rows, then they contained fewer columns.  

The second reason is that SD2011 contain many properties of real data, including missing, outlier, `messy' values, and generated variables.  Previous evaluations used clean data from Census \cite{little2022comparing} or machine learning benchmarks (Kaggle, UCI, OpenML) \cite{dankar2021fake}.  Clean or even simulated data can be valuable, especially for evaluation purposes, but real data present challenges for SDGs that are not otherwise understood.  For example, some SDGs cannot be used on data with missing values.  Therefore, the ability to synthesize missing values is a stage of development that SDGs must go through before they can be applied to real data.\footnote{Early versions CTGAN could not be used on data with missing values (\url{https://github.com/sdv-dev/CTGAN/issues/39}).}  

\subsection{Synthetic data generators (SDGs)}

{\bf synthpop} (Version 1.8.0) \cite{nowok2016synthpop} is an \textsf{R} package that implements parametric and ML based models (classification and regression trees (CART) and random forests) to generate synthetic data. CART models are the default. synthpop follows a sequential process, where the first variable to be synthesized is generated by drawing new values from the marginal distribution of this variable (either by drawing from a parametric distribution or by sampling from the empirical distribution), and the subsequent variables are synthesized one at a time, always conditioning on those variables that have been synthesized in earlier steps. 

%The advantage is one can easily create synthetic data with little tuning.  The disadvantage is that CART, like most tree-based models struggle computationally with categorical variables that contain many values  \cite{little2022comparing}. 


Synthetic data are generated using CART by finding recursive binary splits trying to maximize the homogenity of the values of the dependent variable in the two leaves generated by the split. To find the best possible splits, CART models search overall variables in the dataset. For each variable all possible splits are evaluated and the split that maximizes the homogeneity across all splits and all variables is selected. For continuous and ordered categorical variables the number of splits that needs to be evaluated is $k-1$, where $k$ is the number of unique values in the variable. This is because each value is considered as a threshold with all values less than the threshold ending up in the left leaf while all other values will end up in the right leaf. 

For unordered categorical variables, the number of splits that need to be considered is $2^{L-1}-1$, where $L$ is the number of categories, i.e., the number splits grow exponentially with the number of categories. To illustrate, imagine one categorical variable with three values (a, b, and c).  There are $2^2-1=3$ possible options to split this variable: (1 = a; 0 = b,c), (1 = b; 0 = a,c), (1 = c; 0 = a,b). With six categories, we already need to consider 31 splits, which still doesn't pose a problem computationally. However, if there are 20 categories, then 524,288 splits need to be considered.  The computational burden can be substantial with even a few categorical variables with a large number of categories.

{\bf DataSynthesizer} (Version 0.1.13) \cite{ping2017datasynthesizer} is a Python package that implements the PrivBayes algorithm \cite{zhang2017privbayes}.  PrivBayes is designed to address the challenges associated with a differentially private method for releasing synthetic data outputs from high-dimensional real data inputs.  To do this, the package implements a Bayesian network model that directly specifies the joint distribution of the data.  This strategy reduces the computational burden found in high-dimensional data, but assumes that all variables are categorical.

To generate synthetic data from a Bayesian network, the first step is to specify a graphical model (a directed acyclical graph (DAG)) that represents how and in what way the different variables are related to each other. DataSynthesizer doesn't require this model structure as input, instead it tries to estimate the optimal structure given the data. The only parameter that can be provided is the number of parents that should be considered for the model ($k$). The more parents, the more complex relationships between the variables. After specifying the model and estimating its parameters from the original data, synthetic data are generated by sampling new values based on the conditional probabilities from the model's conditional probability table (CPT).  

For example, imagine data with four columns (variables) with categorical values: age (young, middle, old), education (less than secondary, secondary, and more than secondary), gender (M and F), and income (low, middle, and high).   The number of observations (or rows) are not relevant because the data are transformed into a frequency table with one cell for each unique combination of groups.  In this example, there are 54 cells  ($3 \times 3 \times 2\times 3$).  If we assume that age, education, and gender are the parents of income, then each value in the CPT represents the conditional probability of each income category given the states of the other three variables. The algorithm calculates the probabilities based on the frequencies from all possible combinations of the variables.  To make this model tractable for high dimensional data, the graph structure enforces conditional independence between some of the variables, reducing the number of parameters that need to be estimated. Once the model is defined and parameters are estimated, the Bayesian network generates synthetic data by sampling  new values using the estimated probabilities.  

{\bf CTGAN} (Version 1.9.0) \cite{ctgan} is a Python package that is part of the Synthetic Data Vault package \cite{patki2016synthetic}.  In their original application, generative adversarial networks (GANs) were designed to create synthetic images  \cite{goodfellow2014generative}, but the approach was later adapted to also create synthetic microdata \cite{park2018data}.  

GANs simultaneously train two neural networks: a generator and a discriminator. The goal of the generator is to create synthetic data that becomes increasingly indistinguishable from original data.  The goal of the discriminator is to get better at distinguishing between original and synthetic data.  This adversarial process goes back and forth until the discriminator cannot distinguish between the original data and the generated data.  GANs are designed to work well with continuous variables, but GANs can struggle modeling relationships between cells and columns because cells in micro data are not informative of neighbors in the same way that pixel cells in a photograph \cite{drechsler202330}. 

To illustrate how GANs generate synthetic data, imagine we have one variable: income (LN) from Census data with a mean of 10 and a standard deviation of 1.  First, the generator network receives random noise vectors as input, typically sampled from a standard normal distribution with mean of 0 and a standard deviation of 1, and sends it to the discriminator to be evaluated.  Second, the discriminator evaluates the synthetic data alongside real data to determine the probability that the generated data is real (1) or fake (0).  If the discriminator determines that the generated data are fake, it sends feedback to the generator in the form of a loss function.  Higher values indicate that it is easier for the discriminator to differentiate between real and fake data.  Third, the generator then updates its parameters based on the loss function and tries again.  The learning rate determines the magnitude of this update.  The higher the learning rate, the larger the adjustments the generator will make to its parameters in response to the feedback.  This back and forth process results in a generator that produces synthetic data with the same statistical properties of the original data.  


\section{Know your data}\label{sec:know_your_data}

SD2011 contain a variety of characteristics found in real data that can present a challenge to synthetic data generators (SDGs). These challenges should be addressed prior to applying a SDG.  However, cleaning the data requires knowledge of the data that is not always available to those with knowledge of a given SDG and may not be easy to detect or follow simple rules.  We use DataSynthesizer to demonstrate the importance of preprocessing the data, but the points raised in this section are applicable to all SDGs.

{\bf Missing values.} In real data, missing values are sometimes coded as either negative values or large positive values (999999).  For example, in the variable \texttt{wkabdur} (Months working abroad in 2007-2011) from SD2011, 97.5\% of all values are -8.  The interpretation is values of -8 represent missing values and only 2.5\% of the units contained in the sample worked abroad between 2007 and 2011.  If these values are not cleaned and coded as missing before applying the SDG or the SDG is informed that these values represent missing values (which is possible for example in synthpop), then the SDG will treat these values as regular values to be included in the synthetic data, which will reduce statistical utility.  For example, if we did not code values of -8 as missing in the original data, then values between -8 and 0 would be created in the synthetic data that do not exist in the original data, as shown in figure \ref{subfig:graph_datasynthesizer_wkabdur}.  

\begin{figure}[ht!]
    \centering        
    \caption{Work abroad duration (in weeks)}
    \resizebox{.9\textwidth}{!}{\includegraphics{../graphs/datasynthesizer/datasynthesizer_wkabdur.pdf}}
    \label{subfig:graph_datasynthesizer_wkabdur}
\end{figure}

{\bf Generated variables.} Generated variables are variables that are deterministic functions of other variables included in the dataset.  In SD2011, there are two generated variables.  The variable \texttt{agegr} is generated from \texttt{age} by classifying the age variable into 6 age groups and the variable \texttt{bmi} is generated from the variables \texttt{height}(cm) and \texttt{weight}(kg) (\texttt{bmi}=\texttt{weight}/$(\texttt{height}/100)^2$).  It is not necessary to synthesize generated variables and doing so can be problematic as inconsistencies might arise in the synthetic data.  For example, in the case of \texttt{agegr}, if we apply the SDG to the original data that still includes \texttt{agegr}, then some synthetic values of \texttt{agegr} would be inconsistent with the synthetic values of \texttt{age}. In Figure \ref{subfig:graph_datasynthesizer_frequency_agegr_errors} the frequency of the age groups derived from the synthetic \texttt{age} variable are shown for each category of \texttt{agegr} (frequencies are based on $m=5$ synthetic datasets). We drop values that match correctly so the graph only shows the mismatches.  For example, for synthetic observations classified in age group 25-34 according to \texttt{agegr}, 165 (0.66\%) observations have a generated synthetic age between 16-24 and 279 (1.12\%) have a generated synthetic age between 35-44.  In total, 7.88\% of all observations would be misclassified.

To avoid these inconsistencies, generated variables should be dropped prior to applying the SDG, and then recreated based on the synthetic values of the underlying variables. However, one needs to be aware of the problem to avoid it. In practical situations, logical constraints between the variables can be much more difficult to identify and it requires good knowledge of the data to avoid implausibilities that subject matter experts would easily detect. 

\begin{figure}[ht!]
    \centering        
    \caption{Synthetic age group does not always equal synthetic age values}
    \resizebox{.9\textwidth}{!}{\includegraphics{../graphs/datasynthesizer/datasynthesizer_frequency_agegr_errors.pdf}}
    \label{subfig:graph_datasynthesizer_frequency_agegr_errors}
\end{figure}

{\bf Outliers.} Outliers can cause problems if they are not modeled carefully. For example, in the case of \texttt{bmi}, there is an outlier value of 450 in the original data.  While this value is an obvious error, what would be the implications if it were correct (and thus could not simply be removed before the synthesis)?  If we included the value of 450 in the original data, then some SDGs would create values between 450 and 70 in the synthetic data that do not exist in the original data, as shown in figure \ref{subfig:graph_datasynthesizer_bmi}.  Fortunately, in our case, \texttt{bmi} is a generated variable, which we can drop.  However, the example serves to illustrate that the existence of outlier values can affect the ability of SDGs to create synthetic data with high levels of utility and might also be problematic from a privacy perspective.  


\begin{figure}[ht!]
    \centering        
    \caption{Body mass index (BMI)}
    \resizebox{.9\textwidth}{!}{\includegraphics{../graphs/datasynthesizer/datasynthesizer_bmi.pdf}}
    \label{subfig:graph_datasynthesizer_bmi}
\end{figure}

{\bf Messy values.} In real data, variables include messy values, such as spikes in variables that otherwise can be treated as being continuous.  An example in SD2011 is the variable \texttt{nofriend} (number of friends).  In the original data, the variable \texttt{nofriend} appears to be normally distributed below 10, but then clusters at values of 10, 15, 20 and groups of 10 up to the maximum 99.  This phenomenon is sometimes referred to as `spikey', discontinuous, or semi-continuous distributions.  As shown in figure \ref{subfig:graph_datasynthesizer_nofriend} and discussed in more detail below, errors and spikes can pose a problem for statistical utility because unless these spikes are explicitly modeled, most SDGs tend to smooth these spikes.

\begin{figure}[ht!]
    \centering        
    \caption{Number of friends}
    \resizebox{.9\textwidth}{!}{\includegraphics{../graphs/datasynthesizer/datasynthesizer_nofriend.pdf}}
    \label{subfig:graph_datasynthesizer_nofriend}
\end{figure}

To illustrate the importance of pre-processing the data, we use three versions of the original data to create synthetic data and evaluate the utility of each generated dataset using the pMSE.  SD2011(a) is the raw data.  SD2011(b) codes all negative values that indicate missing values in numeric variables and all empty values in character variables as missing.  SD2011(c) drops the two generated variables (\texttt{agegr} and \texttt{bmi}) and then recreates them from the synthetic values. Using Datasynthesizer as our SDG and 2 parents, the pMSE changes from 0.13 to 0.07 for SD2011(b) to SD2011(c). The improvements in utility as measured by the pMSE are substantial. In fact, when experimenting with the different tuning parameters of the different synthesizers, we found that none of the tuning parameters had such a strong impact on utility as pre-processing the data (results not reported).   


\begin{figure}[ht!]
    \centering        
    \caption{pMSE for DataSynthesizer by dataset and parents}
    \resizebox{.9\textwidth}{!}{\includegraphics{../graphs/datasynthesizer/datasynthesizer_fidelity_optimize_dataset_parents_compare.pdf}}
    \label{subfig:datasynthesizer_fidelity_optimize_dataset_parents_compare}
\end{figure}

%All SDGs were developed with particular data sets or particular problems to solve.  We show what happens if you apply SDGs to real data in order to illustrate that each contain advantages and disadvantages that make the more or less appropriate for any given data set or purpose.  

\section{Know your generator}\label{sec:know_your_generator}

In this section we discuss the importance of knowing the details of the underlying methodology of the SDG to avoid pitfalls or to at least be aware for which analysis tasks the generated data might offer reasonable analytical validity and for which not. We illustrate this point by providing an example of a methodological aspect for each synthesizer that has important impacts on the synthesis, but that might not be immediately obvious when only considering the general methodology of the SDG.  %Some SDGs that perform well on one or another measure of utility, do not perform well on another. Therefore, users interested in creating synthetic data must trade-off what utility measures are important for their research purpose.


%It can be as important to pre-process or clean the data prior to applying a given SDG as it is to tune a SDG.  To illustrate this, we use three versions of the original data to create synthetic data and compare utility.  SD2011(a) is the raw data.  SD2011(b) codes all negative values that indicate missing values in numeric variables as missing, and all empty values in character variables as missing.  SD2011(c) drops the two generated variables (\texttt{agegr} and \texttt{bmi}) and then recreates them from the synthetic values.  All SDGs were developed with particular data sets or particular problems to solve.  We show what happens if you apply SDGs to real data in order to illustrate that each contain advantages and disadvantages that make the more or less appropriate for any given data set or purpose.  


\subsection{synthpop} 

%We apply synthpop to SD2011(c) using default values for all hyperparameters.  A feature of CART-based models is that they can produce synthetic data with high levels of statistical utility \cite{little2022comparing,dankar2021fake,drechsler2011empirical}, but they are not computationally efficient with high dimensional data.  While CART-based models are well described elsewhere \cite{breiman2017classification,reiter2005using}, the important point we want to make is that the number and order of categorical variables with a large number of unique values in data sets can affect the duration in time required to synthesize data \cite{raab2017guidelines}.  The result is a trade-off is between statistical utility and computational efficiency in high dimensional data.


For the sequential modeling approach that is used with synthpop, where each variable that has been synthesized previously is used as a predictor in all subsequent synthesis models, ordering of the variables can have a very strong impact on the run-time of the synthesizer. If a variable with many categories is synthesized early during the synthesis process, it will always be used as a predictor for all other synthesis models imposing a high computational burden on the synthesizer. On the other hand, if the variable is the last variable to be synthesized, it will never be used as a predictor, considerably speeding up the synthesis process. This is why it is often recommended in practice to synthesize categorical variables with many categories last when using CART models \cite{raab2017guidelines}.  Problems arise if there are a sufficient number of variables with a large number of unique values.  For example, if a Census data set contained variables for 3-digit ISO country code, 4-digit ISCO codes (occupation), and 4-digit ISIC codes (industry).  In this scenario, it would be difficult to avoid computational problems through ordering.  

%The process of splitting categorical variables affects duration time in two ways.  First, the more categories you have, the more options there are to try.  More options require more time, by definition.  Second, the splitting process has to be done each time the variable is used as a predictor.  If a variable with a large number of unique values is ordered early, then the splitting process has to be done for each of the subsequent variables in the model.  

Other solutions exist, but limitations remain.  One option is to aggregate categorical variables with a large number of unique values.  However, avoiding the information loss from aggregation is typically one of the reasons to rely on synthetic data to begin with.  Alternatively, the categorical variable can be used to stratify the data and run separate synthesis models within each stratum. Obviously, this will only be an option if the sample sizes in each stratum are still large enough to allow suitably reach synthesis models within each stratum.

To illustrate the problem with categorical variables, we examine the duration in time required to create synthetic data using synthpop, i.e. computational efficiency, as shown in table \ref{table:table_sd2011_duration}.  If we load the raw SD2011 into \textsf{R} as a .csv file as we normally do with a SDG, then synthpop requires 2,132 seconds (35 minutes and 30 seconds), but if we load SD2011 into \textsf{R} from the synthpop package, then synthpop requires 5,474 seconds (91 minutes).  The difference in duration within synthpop is explained by two variables: \texttt{eduspec} and \texttt{wkabdur}.  Both variables have a large number of unique values (28 and 33, respectively). Within the synthpop package \texttt{wkabdur} is coded as a character variable, which implies that CART automatically treats it as an unordered categorical variable.  However, if SD2011 is loaded into \textsf{R} from a .csv file, then \texttt{wkabdur} is treated as a numeric variable.  When we treat \texttt{wkabdur} as a numeric variable and place \texttt{eduspec} at the end of the synthesis chain, synthpop requires less than 15 seconds, regardless of how SD2011 is loaded.  %Therefore, synthpop is a computationally efficient SDG with proper preprocessing.

However, the issue reveals a more general problem that synthpop is sensitive to the number (and order) of categorical variables with large values.  To illustrate this, we added one, two, or three categorical variables with 20, 25, or 30 unique random categorical values to the end of the original SD2011 data.  Results are presented in table \ref{table:table_sd2011_duration}. Duration times for synthpop increase with additional variable and number of unique values. %Under these conditions, it is difficult to imagine applying synthpop to data containing many categorical variables with many categories.
Table \ref{table:table_sd2011_duration} also reveals that the other synthesizers are less sensitive to the number of categorical variables comprising many categories.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Tables
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
    \centering
    \caption{Duration (in seconds)}
    \rowcolors{1}{white}{lightgray}
    \resizebox{\textwidth}{!}{\input{../tables/table_sd2011_duration_v2.tex}}
    \label{table:table_sd2011_duration}
    \\
    \raggedright
    \tiny{Notes: synthpop (csv) indicates that SD2011 was loaded into R from a csv file.  synthpop (package) indicates that SD2011 was loaded into R from the synthpop package.  CTGAN is estimated with 600 epochs.  DataSynthesizer is estimated with 2 parents.}
\end{table}

%Therefore, synthpop is less computationally efficient as the data dimensions increase. 
%The trade-off of computational inefficiency is high levels of statistical utility.  We examine parameter estimates and confidence interval overlap (CIO) for two regression models (OLS and LPM), as shown in figure \ref{fig:utility_compare_cio}.  The overlap is higher in the OLS model (ROE = 0.59) compared to the LPM model (ROE = 0.40).  The difference is explained when we examine the parameter estimates.  In the LPM model, synthpop over estimates the effect of education level on smoking for two values (less than secondary and secondary) as positive and significant.  In the original data, they are positive and insignificant.  The conclusion from synthetic data would be that levels of education affect smoking, but the conclusion from  the original data is that smoking is only related to one level of education (vocational or grammar).  Therefore, even with high levels of CIO, synthetic data from synthpop is not a substitute for original data.

\subsection{DataSynthesizer}\label{subsec:know_your_generator_datasynthesizer}

The algorithm used by DataSynthesizer assumes that all variables are discrete, like most Bayesian network models \cite{young2009using}.  Making this assumption can simplify the modeling task required to represent complex relationships.  Relatedly, this increases computational efficiency and allows the model to be applied to high dimensional data because the information in the data can be reduced without information loss to the full conditional probability table. 

When dealing with continuous variables, Bayesian Networks typically categorize or discretize these variables prior to estimating the model.  This is often done by binning the continuous variable and using the binned variable  for the model.  Once synthetic data have been generated based on this model, values for continuous variables are re-encoded within each bin. Given that the Bayesian Network implicitly assumes conditional independence of values the within a bin and all the other variables contained in the model, it is critical for the analytical validity of the generated data how this encoding step is implemented. DataSynthesizer implements are very basic encoding strategy sampling values from a uniform distribution  within each bin.\footnote{We note that the DataSynthesizer paper states \cite{ping2017datasynthesizer}, ``when invoked in correlated attribute mode, DataDescriber samples attribute values in appropriate order from the Bayesian network.''  However, in the code, it seems that data are created by uniform sampling within a bin (\url{https://github.com/DataResponsibly/DataSynthesizer/blob/90722857e7f6ed736aaa25068ecf9e77f34f896a/DataSynthesizer/datatypes/AbstractAttribute.py\#L125}).}  

To demonstrate the negative effects of this strategy, we refer to figures \ref{subfig:graph_datasynthesizer_bmi} and \ref{subfig:graph_datasynthesizer_nofriend} again, which compare frequency counts for the two variables \texttt{bmi} (body mass index) and  \texttt{nofriend} (number of friends).  Since DataSynthesizer samples from a uniform distribution within the bins, synthetic values of \texttt{bmi} are not normal as they are in the original data. For  \texttt{nofriend}, DataSynthesizer underestimates the frequencies for small counts and does not preserve the spikes in the data for the larger counts, as it distributes the frequencies equally within the bins. 
%the synthetic data underestimates high values between 0 and 10, and overestimates low values between 10 and 15.  Above values of 15, synthetic data miss the spikes at 20, 30, 40, 50, etc.  The consequence of categorizing continuous variables into bins and then re-encoding values within a given bin from a uniform distribution reduces statistical utility.

%We note that this simplistic encoding strategy also has strong negative impacts on multivariate relationships, since as discussed above, within bins the approach implies conditional independence between the continuous variable and all the other variables. The consequences of this assumption are evident in figure \ref{fig:utility_compare_cio}, where most of the regression coefficients show attenuation bias for DataSynthesizer.
%\jd{I am not sure if we want to keep this as we don't refer to these regressions any more for any of the other synthesizers. If we keep it, we might as well only keep the results for DataSynthesizer. But if we keep it, we need to say something about these models somewhere or at least refer to the appendix assuming the models are described there.}

Of course, the utility of the synthetic data could be improved by increasing the number of bins. However, increasing the number of bins will increase the computational complexity. Besides, setting the number of bins too high would lead to unstable model estimates, as a very large number of parameters would need to be estimated from the data. %The trade-off of reduced statistical utility is increased computational efficiency, as shown in table \ref{table:table_sd2011_duration}.  For the raw data, DataSynthesizer requires 245 seconds (4 minutes).  At maximum, with three additional variables, each of which contains 30 unique categorical values, DataSynthesizer requires 424 seconds (7 minutes).  Therefore, DataSynthesizer is relatively insensitive to the number and type of variables that are included in the data frame.  Under these conditions, it is not hard to imagine applying DataSynthesizer to high dimensional data.

\subsection{CTGAN} 
\jl{Currently, we have no utility measure graph for CTGAN.}

There are two main features of CTGAN that we would like to emphasize.  One, the algorithm assumes that all variables are continuous.  This means that the algorithm transforms categorical variables into continuous variables, where each categorical value occupies its allocated distribution with a Gaussian distribution \cite{patki2016synthetic}.
\jd{It turns out that this is not correct unless SDV does its own data pre-processing that is applied to all methods implemented within SDV. CTGAN treats categorical variables as such, but it turns them into dummy variables (one-hot encoding) and models those instead. I could imagine that this could be problematic as we will have to deal with a very high-dimensional and sparse matrix if we have many variables with lots of categories, but this is only speculation at this point. The methodology for dealing with categorical variables is described in their paper, but as so often with these 8 page CS papers, I am struggling to fully understand it as I feel there is important information missing and also of course, because I don't know enough about neural networks to fully understand their specification and which problems this might imply. I hope that Marcel can have a look at the paper (the entire methodology is described on two pages) and tell us, which problems might arise.}  

Two, as with GANs in general, training and tuning hyperparameters are often central in affecting the quality of the synthetic data output.  
\jd{As long as we are unable to show that, I would be hesitant to include this statement here. Especially since we argue that we want to illustrate these potential problems in this section. Did you look at other utility metrics such as the CIO or computing pMSE always using only two variables in the model as implemented in utility.tables() in synthpop?}
CTGAN contains a number of hyperparameters that one can use to tune the model.  We tune the hyperparameters in three main ways.  First, we maintain a constant number of steps (3.000), but allow the batch size to vary, as shown in figure \ref{fig:ctgan_fidelity_optimize_batch_size}.  Second, we maintain a constant batch size (500), but allow the steps to vary, as shown in figure \ref{fig:ctgan_fidelity_optimize_epochs}.  Third and finally, we vary the dimensionality of the generator/discriminator networks and the embedding dimension, as shown in figure \ref{fig:ctgan_fidelity_optimize_dimensions}.  In our data, tuning these hyperparameters makes little difference (pMSE $\approx$ 0.16).  Given that, we maintain default values for all hyperparameters with the exception of epochs, which we increase to 600.  We are aware of other research where tuning hyperparameters does make a difference in the quality of synthetic data output (as yet unpublished).  One possible explanation for the fact that hyperparameters do not affect the quality of the synthetic data output is that our data are too low dimensional for the parameters to make a difference.  


\begin{figure}
    \centering        
    \caption{Steps held constant (3,000)}
    \resizebox{.8\textwidth}{!}{\includegraphics{../../ctgan/graphs/ctgan/ctgan_fidelity_optimize_batch_size.pdf}}
    \label{fig:ctgan_fidelity_optimize_batch_size}
\end{figure}

\begin{figure}
    \centering        
    \caption{Batch size held constant (500)}
    \resizebox{.8\textwidth}{!}{\includegraphics{../../ctgan/graphs/ctgan/ctgan_fidelity_optimize_epochs.pdf}}
    \label{fig:ctgan_fidelity_optimize_epochs}
\end{figure}
    
\begin{figure}
    \centering        
    \caption{Vary the dimensionality}
    \resizebox{.8\textwidth}{!}{\includegraphics{../../ctgan/graphs/ctgan/ctgan_fidelity_optimize_dimensions.pdf}}
    \label{fig:ctgan_fidelity_optimize_dimensions}
\end{figure}



%Next, we examine parameter estimates and confidence interval overlap (CIO) for two regression models (OLS and LPM), as shown in figure \ref{fig:utility_compare_cio}.  In both models, overlap is negative, meaning the intervals do not overlap.  Between the two, ROE is better in the LPM model compared to the OLS model (-0.420 and -1.48, respectively).  In the OLS model, there is some attenuation toward zero, but in the LPM model the coefficients from synthetic data are upwardly biased.  Either way, coefficients produced by synthetic data from CTGAN are not related to those produced from original data.

%The last utility measure is computational efficiency, as shown in table \ref{table:table_sd2011_duration}.  For the raw data, CTGAN requires 331 seconds (5.5 minutes).  At maximum, CTGAN requires 420 seconds (7 minutes).  Therefore, CTGAN is relatively insensitive to the number and type of variables that are included in the data frame.  

Finally, we want to emphasize that  while CTGAN does not produce data with high levels of utility that does not mean GANs must be bad in general.  CTGAN is not the only GAN in Synthetic Data Vault and multiple other GANs exist.  %We note a disconnect in the literature on synthetic data between computer scientists that often rely on GANs without comparing them to CART and statisticians that compare GANs to CART and find that CART models are superior \cite{drechsler202330}.  We are aware of GANs that produce synthetic data with similar utility as synthpop (as yet unpublished).  
Based on our own experience, it is possible to create a GAN that provides higher levels of utility.  More generally, one must distinguish between the package and the method.  

\section{Conclusion}\label{sec:conclusion}

In contrast to the simplicity of the original idea, and despite decades of progress, the creation of synthetic data remains challenging.%include steps that are neither common knowledge nor described in one place.  The conclusion is that making synthetic data is complicated and requires knowledge that may not always exist in one place.  The result is a trade-off is between statistical utility and computational efficiency in high dimensional data.

One must know the data.  Most research examining synthetic data generators (SDGs) especially in the computer science literature uses clean data from Census or Machine Learning libraries.  Unlike clean data, real data contain variables with messy values, which can affect the utility of SDGs.  Despite advertisments to the contrary, it is not possible to simply input real data into a SDG and expect high quality synthetic data output without carefully pre-processing the data.  Our results show that utility of synthetic data is a result of both pre-processing the data and tuning a SDG.  

One must know the generator.  For example, synthpop produces synthetic data with the high levels of utility, but the CART method that synthpop uses as the default struggles with computational efficiency with datasets that contain variables with many categories.  %It is difficult to imagine applying synthpop to so-called `big data' with hundreds of variables and millions of observations.  

The other SDGs face different trade-offs.  DataSynthesizer implements a Bayesian network that requires the algorithm to transform continuous variables into categorical variables.  By contrast, CTGAN implements a GAN that requires the algorithm to transform categorical variables into continuous variables.  In both cases, statistical utility is sacrificed for the benefit of computational efficiency.  
\jd{We probably need to rephrase this sentence one we have finished the section on CTGAN.}

Synthetic data is not a substitute for original data \cite{jordon2022synthetic}.  Therefore, prior to making synthetic data, one must decide the purpose.  There is no one size fits all solution and choosing the right SDG is the result of different trade-offs.  



%\subsubsection*{Acknowledgments}
%Use unnumbered third level headings for the acknowledgments. All acknowledgments, including those to funding agencies, go at the end of the paper. Only add this information once your submission is accepted and deanonymized. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{splncs04}
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix
\section{Appendix: Utility measures}\label{appendix:utility_measures}
\setcounter{figure}{0}    
\setcounter{table}{0}    

{\bf The propensity score (pMSE)}  is an utility measure that estimates how well one can discriminate between the original and synthetic data based on a classifier \cite{woo2009global,snoke2018general} and is implemented in \textsf{R} from the synthpop package \cite{nowok2016synthpop}.  This is sometimes called a `broad'\cite{snoke2018general} or `general'\cite{drechsler2009disclosure} measure of utility or `statistical fidelity' \cite{jordon2022synthetic}.  The main steps are to append or stack the original and the synthetic data, add an indicator (1/0) to distinguish between the two, use a classifier to estimate the propensity of each record in the combined dataset being `assigned' to the original data. The pMSE is the mean squared error of these estimated propensities:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
pMSE = \frac{1}{N}\sum_{i=1}^{N}[\hat{p}_i - c]^2
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
where $N$ is the number of records in the combined dataset, $\hat{p}_i$ is the estimated propensity score for record $i$, and $c$ is the proportion of data in the merged dataset that is synthetic (in many cases $c=0.5$). The pMSE can be estimated using all the variables in the dataset, but it can also be computed using subsets of the variables, e.g., all pairwise combinations of variables to evaluate specifically how well the distribution of these variables is preserved. The smaller the pMSE, the higher the analytical validity of the synthetic data.


{\bf Computational efficiency} is the run time (in seconds) required to create one single synthetic dataset from a given SDG.\footnote{In terms of computing power, SDGs were run on a 2022 Macbook Air with 16GB of RAM and an M2 Chip with 8‑Core CPU, 8‑Core GPU, and a 16‑Core Neural Engine.  All SDGs were run one at a time in order to minimize computational power problems from parallelization.}  This is sometimes referred to as `efficiency'\cite{jordon2022synthetic} or `output scalability' \cite{zhang2017privbayes}.  The basic idea is that the algorithms used by SDGs can suffer from the curse of dimensionality.  

\end{document}
