---
title: "Differential privacy in practice (easy version)"
output:
  html_document:
    df_print: paged
  pdf_document: default
date: "2024-06-19"
urlcolor: blue
---

This document implements Damien Desfontaines blog, \`\`A friendly, non-technical introduction to differential privacy''. Here is the [link](https://desfontain.es/blog/differential-privacy-in-practice.html).

<!-- #https://medium.com/@amydata/understanding-differential-privacy-and-laplace-distribution-c9fc9d7117f3 -->


First we load our setup commands:

```{r setup, warning=FALSE, message=FALSE}

 # Set the global seed here
knitr::opts_chunk$set(echo = TRUE)
set.seed(1234)

# load libraries
library(tidyverse)
library(VGAM)

```


## Counting unique users 

Suppose you have a database, and you want to publish how many people in there satisfy a given condition. Say, how many have green eyes? Even if you have many people in your database, you can't just publish the true answer. Let's take a moment to understand why.

With differential privacy, we assume that the attacker knows almost all elements. They only have uncertainty about their target. Say they want to know whether their target has green eyes. If you output the real number $k$, they can compare it with the number of people with green eyes among the people they know. If it's $k-1$, then the target has green eyes. If it's $k$, then the target does not.

So, what do we do? We compute the exact answer, and we add noise. This noise will come from a probability distribution called the Laplace distribution. This distribution has a parameter, its scale, which determines how "flat" it is. It looks like this:

```{r, echo=FALSE, warning=FALSE}

# Define parameters
mean_value <- 0
scale <- 1  # You can adjust the scale (b) as needed

# Generate data for plotting
x_values <- seq(mean_value - 10, mean_value + 10, by = 0.01)
y1_values <- dlaplace(x_values, mean_value, scale) # green eyes

# Create data frame for ggplot
data <- data.frame(x = x_values, 
                   y1 = y1_values
                   )

# Plot the Laplace distribution
ggplot(data) +
  geom_line(aes(x = x, y = y1)) +
  theme_bw() +
  scale_x_continuous(limits = c(-5, 5), 
                     breaks = seq(-4, 4, 2)) +
  theme(axis.title = element_blank(),
        legend.position = "none",
        )
```

So, to get $\epsilon$-differential privacy, we pick a random value according to Laplace(1/$\epsilon$), and we add this noise to the real value. Why does it work? Let's look at the distribution of the number we return, depending on whether the true count is $k=1000$ (blue line, the target doesn't have green eyes) or $k=1001$ (green line, the target has green eyes).

```{r, warning=FALSE, echo=FALSE}

# Define parameters
mean_value <- 1000
scale <- 1  # You can adjust the scale (b) as needed
epsilon <- .1

# Generate data for plotting
x_values <- seq(mean_value - 10, mean_value + 10, by = 0.01)
y1_values <- dlaplace(x_values, mean_value, scale) # not green eyes
y2_values <- dlaplace(x_values, mean_value+1, scale) # green eyes

# Create data frame for ggplot
data <- data.frame(x = x_values, 
                   y1 = y1_values,
                   y2 = y2_values
)

# Plot the Laplace distribution
ggplot(data) +
  geom_line(aes(x = x, y = y1, color = "y1")) +
  geom_line(aes(x = x, y = y2, color = "y2")) +
  scale_color_manual(values = c("y1" = "blue", 
                                "y2" = "green"), 
                     labels = c("Not green eyes", 
                                "Green eyes")) +  
  scale_x_continuous(limits = c(995, 1006), 
                     breaks = seq(996, 1006, 2)) +
  theme_bw() +
  theme(axis.title = element_blank(),
        legend.position = "bottom",
        legend.title = element_blank(),
        legend.key.width = unit(2, "cm"),
        )
```

Let's say the real number is $k=1001$, and after adding noise, we published $1003$. Let's put ourselves in the attacker's shoes. What's the likelihood that the original number was $1001$ vs. $1000$? The hypothesis ``$k=1001$'' is a bit more likely: generating a noise of 2 is more likely than a noise of 3. How much more likely? It turns out that the ratio between these likelihoods is $\dots$ $e^\epsilon$! So the ratio of probabilities of differential privacy is satisfied.  Here's how to quantify this:

## Differential Privacy Likelihood Calculation

To determine the likelihood that the original number was 1000 versus 1001 from the perspective of an attacker, given that a noisy count of 1003 was published, we use the properties of differential privacy, particularly the noise distribution.

### Given Information

-   The noisy count $y = 1003$
-   The possible true counts $x_0 = 1000$ and $x_1 = 1001$
-   The noise follows a Laplace distribution with parameter $b = 1$

### Likelihood Calculation

The probability density function of the Laplace distribution centered at $\mu$ with scale parameter $b = 1/\epsilon$ is:

$$ f(x | \mu, b) = \frac{\epsilon}{2} \exp\left(-|x - \mu|\epsilon\right) $$

Given the noisy count $y = 1003$:

$$ P(y = 1003 | x = 1000) = \frac{\epsilon}{2} \exp\left(-|1003 - 1000|\epsilon\right) = \frac{\epsilon}{2} \exp\left(-3*\epsilon\right) = \frac{\epsilon}{2e^{3\epsilon}} $$

$$ P(y = 1003 | x = 1001) = \frac{\epsilon}{2} \exp\left(-|1003 - 1001|\epsilon\right) = \frac{\epsilon}{2} \exp\left(-2*\epsilon\right) = \frac{\epsilon}{2e^{2\epsilon}} $$

### Likelihood Ratio

To compare the likelihoods, we calculate the ratio of these probabilities:

$$ \frac{P(y = 1003 | x = 1001)}{P(y = 1003 | x = 1000)} = \frac{\frac{\epsilon}{2e^{2\epsilon}}}{\frac{\epsilon}{2e^{3\epsilon}}} = \frac{1}{e^2} \cdot e^3 = e^\epsilon $$

Thus, the likelihood ratio is $e$, which is approximately 2.718.

### Conclusion

Given the published count of 1003, it is approximately 2.718 times more likely that the true count was 1001 than 1000.  We can also transform this into a function.

```{r, warning=FALSE}

# Define the function to calculate the likelihood ratio
calculate_likelihood_ratio <- function(y, x1, x2, b) {
  # Calculate the likelihood for x1
  likelihood_x1 <- (1 / (2 * b)) * exp(-abs(y - x1) / b)
  
  # Calculate the likelihood for x2
  likelihood_x2 <- (1 / (2 * b)) * exp(-abs(y - x2) / b)
  
  # Calculate the likelihood ratio
  likelihood_ratio <- likelihood_x2 / likelihood_x1
  
  return(likelihood_ratio)
}

# Example usage
y <- 1003 # published count 
x1 <- 1000 # possible true count
x2 <- 1001 # possible true count
epsilon <- 1
sensitivity <- 1 # this is always 1 in integer counting queries (if integers were 2,4,6,etc., then sensitivity would be 2)
b <- sensitivity/epsilon # scale parameter

# Calculate the likelihood ratio
likelihood_ratio <- calculate_likelihood_ratio(y, x1, x2, b)
likelihood_ratio

```

## Counting things

OK, so counting unique users was pretty easy. Counting things must also be straightforward, right? Let's say you have a database of suggestions that people sent to your company using a feedback form. You want to publish the number of suggestions you received on a given day. Meanwhile, the attacker wants to get an idea of how many complaints their target published.

What's different about the previous scenario? Can't we just add noise picked from Laplace(1/$\epsilon$) and get $\epsilon$-differential privacy? There's a catch: what if someone sent more than one complaint during one day? Let's say someone was super unhappy and sent five complaints. The other 1000 customers sent one complaint each. The influence of this one disgruntled customer will be larger than before. The two distributions now look like this:

```{r, warning=FALSE, echo=FALSE}

# Define parameters
mean_value <- 1000
scale <- 1  # You can adjust the scale (b) as needed

# Generate data for plotting
x_values <- seq(mean_value - 10, mean_value + 10, by = 0.01)
y1_values <- dlaplace(x_values, mean_value, scale) # unique customer complaints
y2_values <- dlaplace(x_values, mean_value+5, scale) # total customer complaints

# Create data frame for ggplot
data <- data.frame(x = x_values, 
                   y1 = y1_values,
                   y2 = y2_values
)

# Plot the Laplace distribution
ggplot(data) +
  geom_line(aes(x = x, y = y1, color = "y1")) +
  geom_line(aes(x = x, y = y2, color = "y2")) +
  scale_color_manual(values = c("y1" = "blue", 
                                "y2" = "green"), 
                     labels = c("Happy", 
                                "Unhappy")) +  
  scale_x_continuous(limits = c(995, 1010), 
                     breaks = seq(996, 1010, 2)) +
  theme_bw() +
  theme(axis.title = element_blank(),
        legend.position = "bottom",
        legend.title = element_blank(),
        legend.key.width = unit(2, "cm"),
        )
```

The difference between the curves is much larger than before. Their ratio is at most $e^{5\epsilon}$, so using a parameter of $1/\epsilon$ only gives $5\epsilon$-differential privacy. To fix this, we need to add more noise. How much more? It depends on the $maximum$ $contribution$ of one individual user. If the maximum amount of complaints in one day is 5, you must add 5 times the amount of noise. In this example, using Laplace($5/\epsilon$) would give you $\epsilon$-differential privacy.

```{r, warning=FALSE, echo=FALSE}


# Define parameters
mean_value <- 1000
scale <- 5  # You can adjust the scale (b) as needed

# Generate data for plotting
x_values <- seq(mean_value - 10, mean_value + 10, by = 0.01)
y1_values <- dlaplace(x_values, mean_value, scale) # unique customer complaints
y2_values <- dlaplace(x_values, mean_value+5, scale) # total customer complaints

# Create data frame for ggplot
data <- data.frame(x = x_values, 
                   y1 = y1_values,
                   y2 = y2_values
)

# Plot the Laplace distribution
ggplot(data) +
  geom_line(aes(x = x, y = y1, color = "y1")) +
  geom_line(aes(x = x, y = y2, color = "y2")) +
  scale_color_manual(values = c("y1" = "blue", 
                                "y2" = "green"), 
                     labels = c("Unique", 
                                "Total")) +  
  scale_x_continuous(limits = c(995, 1010), 
                     breaks = seq(996, 1010, 2)) +
  theme_bw() +
  theme(axis.title = element_blank(),
        legend.position = "bottom",
        legend.title = element_blank(),
        legend.key.width = unit(2, "cm"),
  )

```

Note that you can't fully automate this process: you need to know what the largest contribution can be. A human, with some knowledge over the process, must make a judgment call. In our case, this could be ``users won't post more than five complaints per day''.

What happens if that judgment call is wrong, and a user later decides to post 10 complaints in one day? To preserve the desired level of privacy, you need to $clamp$ all values to the estimated maximum. In other words, for this outlier user, you would only count 5 complaints in the non-noisy sum.

This process can introduce unexpected bias in the data. So, be careful when estimating the largest contribution! If clamping only happens very rarely, you should be fine.

## Summing or averaging numbers

Let's say each of your users gives your customer service a rating, between -10 and 10. You want to release the average rating. Computing an average is pretty much the same as computing a sum -- add all ratings, then divide by the number of users \footnote{If you want to be extra pedantic, you might also want to add noise to your total number of users. That depends on the flavor of definition that you choose. I'm not going to that level of detail here, and you probably shouldn't either.}. So, what do we do to the sum to achieve differential privacy?

Among all rating options, we only have to consider the worst possible case. How far can the two noise curves be from each other? If the values are all between -10 and 10, the greatest possible difference is 10−(−10)=20. It happens when the attacker tries to determine whether a user voted -10 or 10.

Like in the previous example, you have to add noise of Laplace($20/\epsilon$) to get $\epsilon$-differential privacy. And just as before, you need to check that each value is between your theoretical minimum and maximum. If you find an anomalous value, e.g. lower than the minimum, you need to clamp it to the minimum before adding it to the sum.

In some cases, estimating these minimum or maximum values is difficult. For example, if you're computing the average salary in a large group of people, how should you estimate the upper salary limit? I don't see that problem as a usability flaw of differential privacy. Rather, it suggests that averages are not a meaningful metric in the presence of outliers. Removing these outliers is a good idea for both accuracy and privacy :-)

## Releasing many things at once

OK. What if you don't want to release only one statistic, but many of them? Can you add noise to each of them and be fine? Well $\dots$ it depends. The main question you have to ask yourself is: what is the maximum influence of one individual? There are two distinct possibilities.

#### 1. Statistics are about different people

Suppose you want to release the number of users you have depending on their age ranges: 20-29, 30-39, 40-49, etc. 

Each user will have an influence in at most one of these categories. Either someone is in a given age range, either they're in another one. This situation often appears when you're trying to release a $histogram$:

```{r, warning=FALSE, echo=FALSE}

# Create a sample dataframe with age data
df <- data.frame(age_group = c("10-19","20-29","30-39","40-49","50-59","60-69","70-79","80-89"),
                 n = c(0,10,12,8,7,4,1,0)
                 )

# Plot the percent frequency histogram with differential privacy
ggplot(df, aes(x=age_group, y=n)) +
  geom_bar(stat="identity") +
  scale_y_continuous(breaks = seq(0,12,2)) +
  theme_bw() +
  theme(axis.title = element_blank(),
        legend.position = "bottom",
        legend.title = element_blank(),
        legend.key.width = unit(2, "cm"),
  )
```

When you're in this case, you can safely add Laplace noise of scale 1/$\epsilon$ to each bucket count. There is no problematic interaction between buckets. Releasing the entire histogram is still $\epsilon$-differentially private.

```{r, warning=FALSE, echo=FALSE}

# Parameters for the Laplace distribution
location <- 0  # Mean (location parameter)
epsilon <- .1
sensitivity <- 1 # this is always 1 in integer counting queries (if integers were 2,4,6,etc., then sensitivity would be 2)
b <- sensitivity/epsilon # scale parameter


# Generate Laplace noise for each value in n
laplace_noise <- rlaplace(nrow(df), location, b)

# Add the noise to the n column
df$n_noisy <- df$n + laplace_noise

# Plot the percent frequency histogram with differential privacy
ggplot(df, aes(x=age_group, y=n_noisy)) +
  geom_bar(stat="identity") +
  theme_bw() +
  theme(axis.title = element_blank(),
        legend.position = "bottom",
        legend.title = element_blank(),
        legend.key.width = unit(2, "cm"),
  )

```

Pretty easy, right? Note that this histogram looks a bit weird: the counts are not integers, and one count ended up being negative! We can make it a bit less suspicious, by rounding all counts and replacing all negative numbers by 0.

```{r, warning=FALSE, echo=FALSE}
# Add the noise to the n column
df$n_noisy <- df$n + laplace_noise
df$n_noisy_postprocess <- ifelse(df$n_noisy<0,yes = 0,round(df$n_noisy))

# Plot the percent frequency histogram with differential privacy
ggplot(df, aes(x=age_group, y=n_noisy_postprocess)) +
  geom_bar(stat="identity") +
  theme_bw() +
  theme(axis.title = element_blank(),
        legend.position = "bottom",
        legend.title = element_blank(),
        legend.key.width = unit(2, "cm"),
  )

print(df)
```

This type of post-processing is allowed, thanks to a nifty property of differential property. If you take differentially private data, and make it go through a fixed transformation, you still get differential privacy \footnote{If you're wondering why this is true, here's a super short proof. If there was a post-processing function that would break the differential privacy property… The attacker could run it too, and distinguish between two outcomes. But it's impossible, because differential privacy forbids it :-)}. Convenient!

#### 2. One person is in multiple statistics 

OK, what if you're releasing multiple statistics, but this time, they might all be about the same user? Let's say that you want to publish how many of your users $\dots$

- are younger than 35;
- are using an iOS device;
- are colorblind;
- have started using your app less than a month ago.

The same user could be in all those categories! In this scenario, you can't add Laplace noise of scale $1/\epsilon$ to each count and get ε-differential privacy. Instead, you have to consider each count as a separate data release. Thus, if you have $C$ different counts, you have to add Laplace noise of scale $C/\epsilon$ to each of them. Each independent release will be $\epsilon/C$-differentially private. And we can now use the composition property of differential privacy! This allows us to conclude that the entire release is $\epsilon$-differentially private.

This works for any kind of statistics, not just unique counts. Want to release several pieces of information? Count the maximum influence of one single person, and ``split'' your $\epsilon$ between each data release. This ε is called your privacy budget: you choose $\epsilon_1, \dots, \epsilon_C$ whose sum is ε, and you release the i-th statistic with $\epsilon_i$-differential privacy. This solution is more flexible than simply using $\epsilon/C$-differential privacy on each statistic. If one statistic is more important, or more sensitive to noise, you can attribute more budget for it. The larger the budget portion, the lower the noise. Of course, you will have to add more noise to the other values.

## Traps to avoid

The mechanisms we saw today are pretty straightforward. But anonymization is full of traps: there are still a number of things that can go wrong.

- When summing or averaging numbers, clamping them to the minimum and maximum is essential. Otherwise, all guarantees fly out the window.
- Pay attention how you implement this clamping in practice. Special values like NaN can lead to surprising behavior.
- Of course, you're not allowed to cheat. You have to choose your privacy strategy in advance, then apply it, and release the noisy data. You can't double-check that it's accurate enough. Otherwise, you skew the randomness, and you lose the privacy guarantees.
- When releasing histograms, it's important to choose each category in advance. If you have to look at your data to know what your categories are, you have to use more subtle methods.

This last point is often a problem in practice. For example, say that you want to count how many times each word was used in customer complaints. You can't make a definite word list: people could use words that you didn't predict, or make typos. In that case, the technique I described for histograms doesn't work. 