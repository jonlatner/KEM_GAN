# Load synthetic data ----
df_sds_10 <- read_csv(paste0(synthetic_data,"synthetic_cart_10.csv"))
sds <- syn(df_ods, m = 1, seed = my.seed)
df_sds_1 <- sds$syn
# Generate frequency lists ----
# df_ods
df_frequency <- df_ods
df_frequency$combine <- paste(df_frequency$var1, df_frequency$var2, df_frequency$var3, df_frequency$var4, sep = "")
df_frequency <- df_frequency %>%
select(-matches("var"))
df_frequency_ods <- as.data.frame(table(df_frequency))
df_frequency_ods_unique <- df_frequency_ods %>%
filter(combine == "1111")
df_frequency_ods_unique <- df_frequency_ods_unique$Freq
df_frequency_ods_unique
# df_sds_1
df_frequency_sds_1 <- df_sds_1
df_frequency_sds_1$combine <- paste(df_frequency_sds_1$var1, df_frequency_sds_1$var2, df_frequency_sds_1$var3, df_frequency_sds_1$var4, sep = "")
df_frequency_sds_1 <- df_frequency_sds_1 %>%
select(-matches("var"))
df_frequency_sds_1 <- as.data.frame(table(df_frequency_sds_1)) %>%
filter(combine == "1111")
sds_1_unique <- df_frequency_sds_1$Freq
sds_1_unique
# df_sds_10
df_frequency_ods$n <- 0
df_frequency_ods$type <- "original"
df_frequency_ods$n <- 0
df_frequency_sds_10 <- rbind(df_frequency_ods,df_sds_10) %>%
pivot_wider(names_from = "n", values_from = "Freq")  %>%
filter(combine == "1111") %>%
mutate(across(everything(), ~ replace_na(.x, 0))) %>%
pivot_longer(
cols = `0`:`10`,        # the range of columns to pivot
names_to = "value",     # new column that stores the old column names
values_to = "count",     # new column that stores the numbers
) %>%
filter((type == "original" & value == 0) | (type == "synthetic" & value > 0))
df_frequency_sds_10 %>% print(n=22)
df_frequency_sds_10_unique <- as.vector(df_frequency_sds_10$count)
df_frequency_sds_10_unique
# Disclosure measures ----
# create summary table
t1 <- multi.disclosure(sds, df_ods, print.flag = FALSE, plot = TRUE, keys = c("var1", "var2", "var3"), target = "var4")
ident = print(t1, plot = FALSE, to.print = "ident")
attrib = print(t1, plot = FALSE, to.print = "attrib")
ttest <- print(t1, plot = FALSE, to.print = "allCAPs")
ttest <- print(t1, plot = FALSE, to.print = "TCAP")
ttest
df_risk <- data.frame(
data = c("Original", "Synthetic"),
unique = c(df_frequency_ods_unique,sds_1_unique),
identity = c(t1$ident.orig,t1$ident.syn),
attribute = c(t1$attrib.table$attrib.orig, t1$attrib.table$attrib.syn)
)
df_risk
# Create the xtable object
latex_table <- xtable(df_risk)
colnames(latex_table) <- c("Data", "Unique", "Identity Risk ($repU$)", "Attribute Risk ($DiSCO$)")
print.xtable(latex_table,
include.rownames = FALSE,
floating = FALSE,
booktabs = TRUE,
sanitize.text.function = identity,
file = paste0(tables,"table_disclosure_risk_1.tex"))
# Create 10 synthetic data sets ----
df_sds <- syn(df_ods, m = 10)
for (c in 1:10) {
print(c)
# Create fake synthetic data
sds <- syn(df_ods, m = 1, seed = my.seed)
df_sds$syn[[c]] <- sds$syn
# create seed
my.seed = my.seed + 1
}
# create summary table
t1 <- multi.disclosure(df_sds, df_ods, print.flag = FALSE, plot = TRUE, keys = c("var1", "var2", "var3"), target = "var4")
df_risk <- data.frame(
data = c("Original", "Synthetic"),
identity = c(t1$ident.orig,t1$ident.syn),
attribute = c(t1$attrib.table$attrib.orig, t1$attrib.table$attrib.syn)
)
df_risk
# Table ----
t1 <- disclosure(df_sds, df_ods, print.flag = FALSE, plot = TRUE, keys = c("var1", "var2", "var3"), target = "var4")
repU <- t1$ident$repU
average_row <- mean(repU) # calculate average row across 10 synthetic data sets
repU <- c(0, repU, average_row)
DiSCO <- t1$attrib$DiSCO
average_row <- mean(DiSCO) # calculate average row across 10 synthetic data sets
DiSCO <- c(0, DiSCO, average_row)
df_frequency_sds_10_unique <- c(df_frequency_sds_10_unique, NA)
# create table
df_risk <- data.frame(
data = c("Original", "Synthetic 1", "Synthetic 2", "Synthetic 3", "Synthetic 4", "Synthetic 5", "Synthetic 6", "Synthetic 7", "Synthetic 8", "Synthetic 9", "Synthetic 10", "Average"),
unique = df_frequency_sds_10_unique,
identity = c(repU),
attribute = c(DiSCO)
)
df_risk
# Create the xtable object
latex_table <- xtable(df_risk,align = "llrr")
latex_table <- xtable(df_risk,align = "llrrr")
colnames(latex_table) <- c("Data", "Unique", "Identity Risk ($repU$)", "Attribute Risk ($DiSCO$)")
print.xtable(latex_table,
include.rownames = FALSE,
# include.colnames = FALSE,
floating = FALSE,
booktabs = TRUE,
sanitize.text.function = function(x) {x},
file = paste0(tables,"table_disclosure_risk_10.tex"),
add.to.row = list(pos = list(nrow(latex_table) - 1),
command = "\\midrule \n"))
# Load packages
library(ggplot2)
# Number of candidates
K <- 16
# Uniform prior
prior <- rep(1/K, K)
# Example likelihoods (toy numbers, sum not required to be 1)
likelihood <- c(0.030, 0.040, 0.050, 0.040,
0.030, 0.060, 0.220, 0.040,
0.050, 0.080, 0.060, 0.050,
0.070, 0.060, 0.030, 0.030)
# Step 1: Compute unnormalized posterior = prior * likelihood
unnormalized <- prior * likelihood
# Step 2: Normalize to sum to 1
posterior <- unnormalized / sum(unnormalized)
# Combine into a table
posterior_table <- data.frame(
candidate = paste0("c", 1:K),
prior = round(prior, 4),
likelihood = round(likelihood, 3),
posterior = round(posterior, 3)
)
print(posterior_table)
# MAP estimate (most likely candidate)
map_candidate <- posterior_table$candidate[which.max(posterior)]
map_prob <- max(posterior)
cat("\nMAP choice:", map_candidate, "with posterior probability", map_prob, "\n")
# Visualization
ggplot(posterior_table, aes(x = candidate)) +
geom_bar(aes(y = prior), stat = "identity", fill = "steelblue", alpha = 0.6) +
geom_bar(aes(y = posterior), stat = "identity", fill = "darkorange", alpha = 0.6) +
labs(y = "Probability",
title = "Bayesian Disclosure Risk: Prior vs Posterior",
subtitle = "Blue = Prior (uniform), Orange = Posterior (updated)") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(tidyverse)
library(synthpop)
# Create Ground Truth (GT: df_ods)
df_ods <- data.frame(
var1 = c(0, 0, 0, 1, 1),
var2 = c(0, 0, 1, 0, 1),
var3 = c(0, 1, 0, 0, 1),
var4 = c(0, 1, 1, 0, 1)  # Target variable (t)
)
# Create a composite key for GT
df_ods$q <- paste(df_ods$var1, df_ods$var2, df_ods$var3, sep = "_")
# Create Synthetic Data (SD: df_sds)
df_sds <- data.frame(
var1 = c(0, 0, 0, 0, 1),
var2 = c(0, 0, 0, 1, 0),
var3 = c(0, 1, 1, 0, 0),
var4 = c(0, 1, 0, 1, 0)  # Target variable (t)
)
# Create a composite key for SD
df_sds$q <- paste(df_sds$var1, df_sds$var2, df_sds$var3, sep = "_")
df_sds_1 <- df_sds
# Create Synthetic Data (SD: df_sds)
df_sds <- data.frame(
var1 = c(0, 0, 0, 0, 1),
var2 = c(0, 0, 0, 1, 0),
var3 = c(0, 1, 1, 0, 0),
var4 = c(1, 1, 1, 1, 1)  # Target variable (t)
)
# Create a composite key for SD
df_sds$q <- paste(df_sds$var1, df_sds$var2, df_sds$var3, sep = "_")
df_sds_2 <- df_sds
t1 <- disclosure(df_sds_1, df_ods, keys = c("var1", "var2", "var3"), target = "var4", print.flag = FALSE)
t2 <- disclosure(df_sds_2, df_ods, keys = c("var1", "var2", "var3"), target = "var4", print.flag = FALSE)
print(t1, to.print = c("attrib"))
print(t2, to.print = c("attrib"))
# Step 1: Identify disclosive keys in SD
disclosive_keys <- df_sds_1 %>%
group_by(q) %>%
summarize(
disclosive = n_distinct(var4) == 1,  # Check if var4 (t) is constant
var4_sd = ifelse(disclosive, unique(var4), NA),  # Get the constant value if disclosive
.groups = "drop"
)
# Step 2: Merge disclosive information into GT (df_ods)
df_merged_1 <- df_ods %>%
left_join(disclosive_keys, by = "q") %>%
mutate(
disclosive = ifelse(is.na(disclosive), FALSE, disclosive),  # Mark non-matching q as not disclosive
correct = disclosive & (var4 == var4_sd)  # Check if t matches
)
# Step 3: Calculate DiSCO
DiSCO_1 <- 100 * mean(df_merged_1$correct, na.rm = TRUE)
# Output DiSCO
DiSCO_1
# Step 1: Identify disclosive keys in SD
disclosive_keys <- df_sds_2 %>%
group_by(q) %>%
summarize(
disclosive = n_distinct(var4) == 1,  # Check if var4 (t) is constant
var4_sd = ifelse(disclosive, unique(var4), NA),  # Get the constant value if disclosive
.groups = "drop"
)
# Step 2: Merge disclosive information into GT (df_ods)
df_merged_2 <- df_ods %>%
left_join(disclosive_keys, by = "q") %>%
mutate(
disclosive = ifelse(is.na(disclosive), FALSE, disclosive),  # Mark non-matching q as not disclosive
correct = disclosive & (var4 == var4_sd)  # Check if t matches
)
# Step 3: Calculate DiSCO
DiSCO_1 <- 100 * mean(df_merged_2$correct, na.rm = TRUE)
# Output DiSCO
DiSCO_1
print(t2, to.print = c("attrib"))
print(t1, to.print = c("attrib"))
# Top commands ----
# Create empty R application (no figures, data frames, packages, etc.)
# Get a list of all loaded packages
packages <- search()[grepl("package:", search())]
# Unload each package
for (package in packages) {
unloadNamespace(package)
}
rm(list=ls(all=TRUE))
library(tidyverse)
library(synthpop)
# Create Ground Truth (GT: df_ods)
df_ods <- data.frame(
var1 = c(0, 0, 0, 1, 1),
var2 = c(0, 0, 1, 0, 1),
var3 = c(0, 1, 0, 0, 1),
var4 = c(0, 1, 1, 0, 1)  # Target variable (t)
)
# Create a composite key for GT
df_ods$q <- paste(df_ods$var1, df_ods$var2, df_ods$var3, sep = "_")
# Create Synthetic Data (SD: df_sds)
df_sds <- data.frame(
var1 = c(0, 0, 0, 0, 1),
var2 = c(0, 0, 0, 1, 0),
var3 = c(0, 1, 1, 0, 0),
var4 = c(0, 1, 0, 1, 0)  # Target variable (t)
)
# Create a composite key for SD
df_sds$q <- paste(df_sds$var1, df_sds$var2, df_sds$var3, sep = "_")
df_sds_1 <- df_sds
# Create Synthetic Data (SD: df_sds)
df_sds <- data.frame(
var1 = c(0, 0, 0, 0, 1),
var2 = c(0, 0, 0, 1, 0),
var3 = c(0, 1, 1, 0, 0),
var4 = c(1, 1, 1, 1, 1)  # Target variable (t)
)
# Create a composite key for SD
df_sds$q <- paste(df_sds$var1, df_sds$var2, df_sds$var3, sep = "_")
df_sds_2 <- df_sds
t1 <- disclosure(df_sds_1, df_ods, keys = c("var1", "var2", "var3"), target = "var4", print.flag = FALSE)
t2 <- disclosure(df_sds_2, df_ods, keys = c("var1", "var2", "var3"), target = "var4", print.flag = FALSE)
print(t1, to.print = c("attrib"))
print(t2, to.print = c("attrib"))
# Step 1: Identify disclosive keys in SD
disclosive_keys <- df_sds_1 %>%
group_by(q) %>%
summarize(
disclosive = n_distinct(var4) == 1,  # Check if var4 (t) is constant
var4_sd = ifelse(disclosive, unique(var4), NA),  # Get the constant value if disclosive
.groups = "drop"
)
# Step 2: Merge disclosive information into GT (df_ods)
df_merged_1 <- df_ods %>%
left_join(disclosive_keys, by = "q") %>%
mutate(
disclosive = ifelse(is.na(disclosive), FALSE, disclosive),  # Mark non-matching q as not disclosive
correct = disclosive & (var4 == var4_sd)  # Check if t matches
)
# Step 3: Calculate DiSCO
DiSCO_1 <- 100 * mean(df_merged_1$correct, na.rm = TRUE)
# Output DiSCO
DiSCO_1
# Step 1: Identify disclosive keys in SD
disclosive_keys <- df_sds_2 %>%
group_by(q) %>%
summarize(
disclosive = n_distinct(var4) == 1,  # Check if var4 (t) is constant
var4_sd = ifelse(disclosive, unique(var4), NA),  # Get the constant value if disclosive
.groups = "drop"
)
# Step 2: Merge disclosive information into GT (df_ods)
df_merged_2 <- df_ods %>%
left_join(disclosive_keys, by = "q") %>%
mutate(
disclosive = ifelse(is.na(disclosive), FALSE, disclosive),  # Mark non-matching q as not disclosive
correct = disclosive & (var4 == var4_sd)  # Check if t matches
)
# Step 3: Calculate DiSCO
DiSCO_1 <- 100 * mean(df_merged_2$correct, na.rm = TRUE)
# Output DiSCO
DiSCO_1
View(df_ods)
print(t1, to.print = c("attrib"))
print(t2, to.print = c("attrib"))
library(tidyverse)
library(synthpop)
# Create Ground Truth (GT: df_ods)
df_ods <- data.frame(
var1 = c(0, 0, 0, 1, 1),
var2 = c(0, 0, 1, 0, 1),
var3 = c(0, 1, 0, 0, 1),
var4 = c(0, 1, 1, 0, 1)  # Target variable (t)
)
# Create a composite key for GT
df_ods$q <- paste(df_ods$var1, df_ods$var2, df_ods$var3, sep = "_")
# Create Synthetic Data (SD: df_sds)
df_sds <- data.frame(
var1 = c(0, 0, 0, 0, 1),
var2 = c(0, 0, 0, 1, 0),
var3 = c(0, 1, 1, 0, 0),
var4 = c(0, 1, 0, 1, 0)  # Target variable (t)
)
# Create a composite key for SD
df_sds$q <- paste(df_sds$var1, df_sds$var2, df_sds$var3, sep = "_")
df_sds_1 <- df_sds
# Create Synthetic Data (SD: df_sds)
df_sds <- data.frame(
var1 = c(0, 0, 0, 0, 1),
var2 = c(0, 0, 0, 1, 0),
var3 = c(0, 1, 1, 0, 0),
var4 = c(1, 1, 1, 1, 1)  # Target variable (t)
)
# Create a composite key for SD
df_sds$q <- paste(df_sds$var1, df_sds$var2, df_sds$var3, sep = "_")
df_sds_2 <- df_sds
t1 <- disclosure(df_sds_1, df_ods, keys = c("var1", "var2", "var3"), target = "var4", print.flag = FALSE)
t2 <- disclosure(df_sds_2, df_ods, keys = c("var1", "var2", "var3"), target = "var4", print.flag = FALSE)
print(t1, to.print = c("attrib"))
print(t2, to.print = c("attrib"))
# Step 1: Identify disclosive keys in SD
disclosive_keys <- df_sds_1 %>%
group_by(q) %>%
summarize(
disclosive = n_distinct(var4) == 1,  # Check if var4 (t) is constant
var4_sd = ifelse(disclosive, unique(var4), NA),  # Get the constant value if disclosive
.groups = "drop"
)
# Step 2: Merge disclosive information into GT (df_ods)
df_merged_1 <- df_ods %>%
left_join(disclosive_keys, by = "q") %>%
mutate(
disclosive = ifelse(is.na(disclosive), FALSE, disclosive),  # Mark non-matching q as not disclosive
correct = disclosive & (var4 == var4_sd)  # Check if t matches
)
# Step 3: Calculate DiSCO
DiSCO_1 <- 100 * mean(df_merged_1$correct, na.rm = TRUE)
# Output DiSCO
DiSCO_1
# Step 1: Identify disclosive keys in SD
disclosive_keys <- df_sds_2 %>%
group_by(q) %>%
summarize(
disclosive = n_distinct(var4) == 1,  # Check if var4 (t) is constant
var4_sd = ifelse(disclosive, unique(var4), NA),  # Get the constant value if disclosive
.groups = "drop"
)
# Step 2: Merge disclosive information into GT (df_ods)
df_merged_2 <- df_ods %>%
left_join(disclosive_keys, by = "q") %>%
mutate(
disclosive = ifelse(is.na(disclosive), FALSE, disclosive),  # Mark non-matching q as not disclosive
correct = disclosive & (var4 == var4_sd)  # Check if t matches
)
# Step 3: Calculate DiSCO
DiSCO_1 <- 100 * mean(df_merged_2$correct, na.rm = TRUE)
# Output DiSCO
DiSCO_1
# Load packages
library(ggplot2)
# Number of candidates
K <- 16
# Uniform prior
prior <- rep(1/K, K)
# Example likelihoods (toy numbers, sum not required to be 1)
likelihood <- c(0.030, 0.040, 0.050, 0.040,
0.030, 0.060, 0.220, 0.040,
0.050, 0.080, 0.060, 0.050,
0.070, 0.060, 0.030, 0.030)
# Step 1: Compute unnormalized posterior = prior * likelihood
unnormalized <- prior * likelihood
# Step 2: Normalize to sum to 1
posterior <- unnormalized / sum(unnormalized)
# Combine into a table
posterior_table <- data.frame(
candidate = paste0("c", 1:K),
prior = round(prior, 4),
likelihood = round(likelihood, 3),
posterior = round(posterior, 3)
)
print(posterior_table)
# MAP estimate (most likely candidate)
map_candidate <- posterior_table$candidate[which.max(posterior)]
map_prob <- max(posterior)
cat("\nMAP choice:", map_candidate, "with posterior probability", map_prob, "\n")
# Visualization
ggplot(posterior_table, aes(x = candidate)) +
geom_bar(aes(y = prior), stat = "identity", fill = "steelblue", alpha = 0.6) +
geom_bar(aes(y = posterior), stat = "identity", fill = "darkorange", alpha = 0.6) +
labs(y = "Probability",
title = "Bayesian Disclosure Risk: Prior vs Posterior",
subtitle = "Blue = Prior (uniform), Orange = Posterior (updated)") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Top commands ----
# Create empty R application (no figures, data frames, packages, etc.)
# Get a list of all loaded packages
packages <- search()[grepl("package:", search())]
# Unload each package
for (package in packages) {
unloadNamespace(package)
}
rm(list=ls(all=TRUE))
# load library
library(synthpop)
library(tidyverse)
#functions
options(scipen=999)
# Set seed for reproducibility
my.seed = 1237
set.seed(my.seed)
# Create simulated data ----
# Number of observations
n <- 1000
# Define the 16 possible combinations of four binary variables
combinations <- expand.grid(y1 = c(0, 1), y2 = c(0, 1), y3 = c(0, 1), y4 = c(0, 1))
# Define c_16 and C_âˆ’16
c_16 <- combinations[16,]
C_minus_16 <- combinations[-16,]
# Initialize the dataset
D <- data.frame(matrix(ncol = 4, nrow = n))
colnames(D) <- c("var1", "var2", "var3", "var4")
# Sample the first 999 observations from C_minus_16 with equal probability
for (i in 1:(n-1)) {
sampled_row <- sample(1:15, 1)
D[i,] <- C_minus_16[sampled_row,]
}
# Set the 1000th observation to c_16
D[1000,] <- c_16
# Convert to data frame and print the first few rows
df_ods <- as.data.frame(D)
# Create synthetic data with numeric variables ----
# my.seed = 1237 # reproduces 1 observation
my.seed = 1238 # reproduces 0 unique observations
# my.seed = 1235 # reproduces 0 unique observations
# my.seed = 1240 # reproduces 3 unique observations
sds <- syn(df_ods, m = 1, seed = my.seed)
t1 <- disclosure.summary(sds, df_ods, print.flag = FALSE, plot = TRUE, keys = c("var1", "var2", "var3"), target = "var4")
# Load packages
library(ggplot2)
# Number of candidates
K <- 16
# Uniform prior
prior <- rep(1/K, K)
# Example likelihoods (toy numbers, sum not required to be 1)
likelihood <- c(0.030, 0.040, 0.050, 0.040,
0.030, 0.060, 0.220, 0.040,
0.050, 0.080, 0.060, 0.050,
0.070, 0.060, 0.030, 0.030)
# Step 1: Compute unnormalized posterior = prior * likelihood
unnormalized <- prior * likelihood
# Step 2: Normalize to sum to 1
posterior <- unnormalized / sum(unnormalized)
# Combine into a table
posterior_table <- data.frame(
candidate = paste0("c", 1:K),
prior = round(prior, 4),
likelihood = round(likelihood, 3),
posterior = round(posterior, 3)
)
print(posterior_table)
# MAP estimate (most likely candidate)
map_candidate <- posterior_table$candidate[which.max(posterior)]
map_prob <- max(posterior)
cat("\nMAP choice:", map_candidate, "with posterior probability", map_prob, "\n")
# Load packages
library(ggplot2)
# Number of candidates
K <- 16
# Uniform prior
prior <- rep(1/K, K)
# Example likelihoods (toy numbers, sum not required to be 1)
likelihood <- c(0.030, 0.040, 0.050, 0.040,
0.030, 0.060, 0.220, 0.040,
0.050, 0.080, 0.060, 0.050,
0.070, 0.060, 0.030, 0.030)
# Step 1: Compute unnormalized posterior = prior * likelihood
unnormalized <- prior * likelihood
# Step 2: Normalize to sum to 1
posterior <- unnormalized / sum(unnormalized)
# Combine into a table
posterior_table <- data.frame(
candidate = paste0("c", 1:K),
prior = round(prior, 4),
likelihood = round(likelihood, 3),
posterior = round(posterior, 3)
)
print(posterior_table)
