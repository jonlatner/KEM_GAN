wage_losses_3 = -1*lag(losses,1)*(do^1),
wage_losses_4 = -1*lag(losses,2)*(do^2),
wage_gains_2 = -1*lag(gains,0)*(do^0),
wage_gains_3 = -1*lag(gains,1)*(do^1),
wage_gains_4 = -1*lag(gains,2)*(do^2),
) %>%
ungroup() %>%
mutate(losses = rowSums(across(c(wage_losses_2, wage_losses_3, wage_losses_4)), na.rm = TRUE),
gains = rowSums(across(c(wage_gains_2, wage_gains_3, wage_gains_4)), na.rm = TRUE),
LI = losses*lo + gains*go
) %>%
select(-matches("losses_"), -matches("gains_"))
print(df_data)
# calculate weights (epsilon) ----
# Calculate variances within each insecurity category
df_data <- df_data %>%
group_by(insecurity) %>%
mutate(
var_LI = var(LI),
var_CSI = var(CSI),
) %>%
ungroup() %>%
mutate(sum_var_LI = sum(var_LI, na.rm = TRUE),
sum_var_CSI = sum(var_CSI, na.rm = TRUE),
epsilon = sum_var_CSI / (sum_var_CSI + sum_var_LI)
)
df_data
library(tidyverse)
df_data <- data.frame(
pid = rep(1:4, each = 4),      # ID for individuals
period = rep(1:4, times = 4),  # Time periods
wages = c(4,12,12,16,          # wages
16,12,12,4,
16,4,4,12,
4,16,16,8
),
geo = 10,                      # NUTS1 geocode
male = 1,                      # gender
age_gr = 2,                    # age 31-40
edu = 3,                       # edu highschool
insecurity = c(1,3,3,4,        # subjective measure of insecurity, here this is wages/4 (4 is most secure, 1 is most insecure)
4,3,3,1,
4,1,1,3,
1,4,4,2
)
)
# Define weights
lo <- 1
go <- 15/16
do <- 0.9
# cluster assignment for each individual ----
# The Reference Group
# 1) the most detailed geographical reference available in the data, NUTS1
# 2) the gender,
# 3) the age in classes (up to 30, 31–40, 41–50, 51–60, over 60)
# 4) the educational level (primary education, lower secondary education, upper secondary education, post-secondary non-tertiary education, and tertiary education).
df_cluster <- df_data %>%
select(geo, male, age_gr, edu) %>%
group_by(geo, male, age_gr, edu) %>%
slice(1) %>%
mutate(cluster = row_number()) %>%
ungroup()
# merge
df_data <- merge(df_data,df_cluster) %>%
select(pid,period,wages,geo,male,age_gr,edu,insecurity,cluster)
# drop cluster variables for this simulation
df_data <- df_data %>%
select(-geo,-male,-age_gr,-edu)
# calculate the cross-sectional component ----
# we calculate it by taking the natural logarithm of the ratio
# between the median of the selected variable within the cluster (at time 0) and the individual's resource level at time 0.
# For example, if we treat the four streams you mentioned as individuals in the same cluster, the median would be calculated from the set (4, 8, 12, 16).
df_cross_sectional <- df_data %>%
group_by(cluster, pid) %>%
filter(row_number()==1) %>%
ungroup() %>%
group_by(cluster) %>%
summarise(median_wages = median(wages)) %>%
ungroup()
df_cross_sectional
# merge
df_data <- merge(df_data,df_cross_sectional,by = c("cluster")) %>%
group_by(pid) %>%
mutate(CSI = log(median_wages/wages)) %>%
ungroup()
df_data
# calculate longitudinal component ----
df_data <- df_data %>%
mutate(ln_wages = log(wages)) %>%
group_by(pid) %>%
mutate(diff = ifelse(row_number()>1, yes=-1*(ln_wages-lag(ln_wages)), no=0),
gains = ifelse(diff<0, yes = diff, no = 0),
losses = ifelse(diff>0, yes = diff, no = 0),
wage_losses_2 = -1*lag(losses,0)*(do^0),
wage_losses_3 = -1*lag(losses,1)*(do^1),
wage_losses_4 = -1*lag(losses,2)*(do^2),
wage_gains_2 = -1*lag(gains,0)*(do^0),
wage_gains_3 = -1*lag(gains,1)*(do^1),
wage_gains_4 = -1*lag(gains,2)*(do^2),
) %>%
ungroup() %>%
mutate(losses = rowSums(across(c(wage_losses_2, wage_losses_3, wage_losses_4)), na.rm = TRUE),
gains = rowSums(across(c(wage_gains_2, wage_gains_3, wage_gains_4)), na.rm = TRUE),
LI = losses*lo + gains*go
) %>%
select(-matches("losses_"), -matches("gains_"))
print(df_data)
# calculate weights (epsilon) ----
# Calculate variances within each insecurity category
df_data <- df_data %>%
group_by(insecurity) %>%
mutate(
var_LI = var(LI),
var_CSI = var(CSI),
) %>%
ungroup() %>%
mutate(sum_var_LI = sum(var_LI, na.rm = TRUE),
sum_var_CSI = sum(var_CSI, na.rm = TRUE),
epsilon = sum_var_CSI / (sum_var_CSI + sum_var_LI)
)
df_data
library(tidyverse)
df_data <- data.frame(
pid = rep(1:4, each = 4),      # ID for individuals
period = rep(1:4, times = 4),  # Time periods
wages = c(4,12,12,16,          # wages
16,12,12,4,
16,4,4,12,
4,16,16,8
),
geo = 10,                      # NUTS1 geocode
male = 1,                      # gender
age_gr = 2,                    # age 31-40
edu = 3,                       # edu highschool
insecurity = c(1,3,3,4,        # subjective measure of insecurity, here this is wages/4 (4 is most secure, 1 is most insecure)
4,3,3,1,
4,1,1,3,
1,4,4,2
)
)
# Define weights
lo <- 1
go <- 15/16
do <- 0.9
# cluster assignment for each individual ----
# The Reference Group
# 1) the most detailed geographical reference available in the data, NUTS1
# 2) the gender,
# 3) the age in classes (up to 30, 31–40, 41–50, 51–60, over 60)
# 4) the educational level (primary education, lower secondary education, upper secondary education, post-secondary non-tertiary education, and tertiary education).
df_cluster <- df_data %>%
select(geo, male, age_gr, edu) %>%
group_by(geo, male, age_gr, edu) %>%
slice(1) %>%
mutate(cluster = row_number()) %>%
ungroup()
# merge
df_data <- merge(df_data,df_cluster) %>%
select(pid,period,wages,geo,male,age_gr,edu,insecurity,cluster)
# drop cluster variables for this simulation
df_data <- df_data %>%
select(-geo,-male,-age_gr,-edu)
# calculate the cross-sectional component ----
# we calculate it by taking the natural logarithm of the ratio
# between the median of the selected variable within the cluster (at time 0) and the individual's resource level at time 0.
# For example, if we treat the four streams you mentioned as individuals in the same cluster, the median would be calculated from the set (4, 8, 12, 16).
df_cross_sectional <- df_data %>%
group_by(cluster, pid) %>%
filter(row_number()==1) %>%
ungroup() %>%
group_by(cluster) %>%
summarise(median_wages = median(wages)) %>%
ungroup()
df_cross_sectional
# merge
df_data <- merge(df_data,df_cross_sectional,by = c("cluster")) %>%
group_by(pid) %>%
mutate(CSI = log(median_wages/wages)) %>%
ungroup()
df_data
# calculate longitudinal component ----
df_data <- df_data %>%
mutate(ln_wages = log(wages)) %>%
group_by(pid) %>%
mutate(diff = ifelse(row_number()>1, yes=-1*(ln_wages-lag(ln_wages)), no=0),
gains = ifelse(diff<0, yes = diff, no = 0),
losses = ifelse(diff>0, yes = diff, no = 0),
wage_losses_2 = -1*lag(losses,0)*(do^0),
wage_losses_3 = -1*lag(losses,1)*(do^1),
wage_losses_4 = -1*lag(losses,2)*(do^2),
wage_gains_2 = -1*lag(gains,0)*(do^0),
wage_gains_3 = -1*lag(gains,1)*(do^1),
wage_gains_4 = -1*lag(gains,2)*(do^2),
) %>%
ungroup() %>%
mutate(losses = rowSums(across(c(wage_losses_2, wage_losses_3, wage_losses_4)), na.rm = TRUE),
gains = rowSums(across(c(wage_gains_2, wage_gains_3, wage_gains_4)), na.rm = TRUE),
LI = losses*lo + gains*go
) %>%
select(-matches("losses_"), -matches("gains_"))
print(df_data)
# calculate weights (epsilon) ----
# Calculate variances within each insecurity category
df_data <- df_data %>%
group_by(insecurity) %>%
mutate(
var_LI = var(LI),
var_CSI = var(CSI),
) %>%
ungroup() %>%
mutate(sum_var_LI = sum(var_LI, na.rm = TRUE),
sum_var_CSI = sum(var_CSI, na.rm = TRUE),
epsilon = sum_var_CSI / (sum_var_CSI + sum_var_LI)
)
df_data
df_data
5.77^3
5.77^2
# Top commands ----
# Create empty R application (no figures, data frames, packages, etc.)
# Get a list of all loaded packages
packages <- search()[grepl("package:", search())]
# Unload each package
for (package in packages) {
unloadNamespace(package)
}
rm(list=ls(all=TRUE))
# load library
library(synthpop)
library(tidyverse)
library(ggh4x) # facet_nested
library(readr)
# FOLDERS - ADAPT THIS PATHWAY
main_dir = "/Users/jonathanlatner/Documents/GitHub/KEM_GAN/latner/projects/simulation/"
data_files = "data_files/"
original_data = "data_files/original/"
synthetic_data = "data_files/synthetic/synthpop/"
graphs = "graphs/"
setwd(main_dir)
#functions
options(scipen=999)
# Set seed for reproducibility
my.seed = 1234
set.seed(my.seed)
# Combinations ----
# Define the 16 possible combinations of four binary variables
combinations <- expand.grid(y1 = c(0, 1), y2 = c(0, 1), y3 = c(0, 1), y4 = c(0, 1))
# Loop ----
df_frequency <- data.frame()
for (c in 1:10) {
for (r in 1:16) {
# create seed
my.seed = my.seed + 1
# Load original data
df_ods <- read.csv(paste0(original_data,"simulated.csv"))
# Drop the last row
df_ods <- head(df_ods, -1)
# Set the last observation to last_record
last_record <- combinations[r,]
print(last_record)
df_ods[1000,] <- last_record
# Create fake synthetic data
sds <- syn(df_ods, m = 1, seed = my.seed, method = "cart", cart.cp = 0.05)
sds <- sds$syn
df_sds <- sds
# Create a frequency table for true original data (unique = 1111)
df_ods_frequency <- df_ods
df_ods_frequency$combine <- paste(df_ods_frequency$var1, df_ods_frequency$var2, df_ods_frequency$var3, df_ods_frequency$var4, sep = "")
df_ods_frequency <- df_ods_frequency %>%
select(-matches("var"))
df_ods_frequency <- as.data.frame(table(df_ods_frequency)) %>%
mutate(type = "original",
n = c,
last_record = paste(last_record$y1, last_record$y2, last_record$y3, last_record$y4, sep = ""))
# Create a frequency table for synthetic data
sds$combine <- paste(sds$var1, sds$var2, sds$var3, sds$var4, sep = "")
sds <- sds %>%
select(-matches("var"))
df_sds_frequency <- as.data.frame(table(sds))
df_sds_frequency$type <- "synthetic"
df_sds_frequency$n <- c
df_sds_frequency$last_record <- paste(last_record$y1, last_record$y2, last_record$y3, last_record$y4, sep = "")
# Combine
df_frequency <- rbind(df_frequency,df_sds_frequency,df_ods_frequency)
}
}
# Save data ----
write.csv(df_frequency, paste0(synthetic_data,"synthetic_attacker_modified_cp.csv"), row.names = FALSE)
# Compare histogram ----
df_frequency <- read_csv(paste0(synthetic_data,"synthetic_attacker_modified_cp.csv"))
df_graph_sds <- df_frequency %>%
filter(type == "synthetic")
df_graph_ods <- df_frequency %>%
filter(type == "original")
df_graph_ods <- unique(df_graph_ods)
df_graph <-
ggplot() +
geom_bar(data = df_graph_ods, aes(x = combine, y = Freq, fill = type), position = position_dodge(width=0.9), stat = "identity") +
geom_boxplot(position = position_dodge(width=0.9), aes(x = combine, y = Freq, fill = type), data = df_graph_sds) +
facet_wrap(~last_record, labeller = "label_both") +
scale_y_continuous(limits = c(0,100), breaks = seq(0,100,25)) +
theme_bw() +
theme(panel.grid.minor = element_blank(),
legend.position = "bottom",
legend.title = element_blank(),
legend.key.width=unit(1, "cm"),
axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5),
axis.title.x = element_blank(),
axis.line.y = element_line(color="black", linewidth=.5),
axis.line.x = element_line(color="black", linewidth=.5)
)
df_graph
ggsave(plot = df_graph, paste0(graphs,"graph_attacker_modified_cp.pdf"), height = 5, width = 10)
ggsave(plot = df_graph, paste0(graphs,"graph_attacker_modified_cp_v2.pdf"), height = 5, width = 5)
# Top commands ----
# Create empty R application (no figures, data frames, packages, etc.)
# Get a list of all loaded packages
packages <- search()[grepl("package:", search())]
# Unload each package
for (package in packages) {
unloadNamespace(package)
}
rm(list=ls(all=TRUE))
# load library
library(synthpop)
library(tidyverse)
library(ggh4x) # facet_nested
library(readr)
# FOLDERS - ADAPT THIS PATHWAY
main_dir = "/Users/jonathanlatner/Documents/GitHub/KEM_GAN/latner/projects/simulation/"
data_files = "data_files/"
original_data = "data_files/original/"
synthetic_data = "data_files/synthetic/synthpop/"
graphs = "graphs/"
setwd(main_dir)
#functions
options(scipen=999)
# Set seed for reproducibility
my.seed = 1234
set.seed(my.seed)
# Combinations ----
# Define the 16 possible combinations of four binary variables
combinations <- expand.grid(y1 = c(0, 1), y2 = c(0, 1), y3 = c(0, 1), y4 = c(0, 1))
# Loop ----
df_frequency <- data.frame()
for (c in 1:10) {
for (r in 1:16) {
# create seed
my.seed = my.seed + 1
# Load original data
df_ods <- read.csv(paste0(original_data,"simulated.csv"))
# Drop the last row
df_ods <- head(df_ods, -1)
# Set the last observation to last_record
last_record <- combinations[r,]
print(last_record)
df_ods[1000,] <- last_record
# Create fake synthetic data
sds <- syn(df_ods, m = 1, seed = my.seed, method = "cart", cart.minbucket = 75)
sds <- sds$syn
df_sds <- sds
# Create a frequency table for true original data (unique = 1111)
df_ods_frequency <- df_ods
df_ods_frequency$combine <- paste(df_ods_frequency$var1, df_ods_frequency$var2, df_ods_frequency$var3, df_ods_frequency$var4, sep = "")
df_ods_frequency <- df_ods_frequency %>%
select(-matches("var"))
df_ods_frequency <- as.data.frame(table(df_ods_frequency)) %>%
mutate(type = "original",
n = c,
last_record = paste(last_record$y1, last_record$y2, last_record$y3, last_record$y4, sep = ""))
# Create a frequency table for synthetic data
sds$combine <- paste(sds$var1, sds$var2, sds$var3, sds$var4, sep = "")
sds <- sds %>%
select(-matches("var"))
df_sds_frequency <- as.data.frame(table(sds))
df_sds_frequency$type <- "synthetic"
df_sds_frequency$n <- c
df_sds_frequency$last_record <- paste(last_record$y1, last_record$y2, last_record$y3, last_record$y4, sep = "")
# Combine
df_frequency <- rbind(df_frequency,df_sds_frequency,df_ods_frequency)
}
}
# Save data ----
write.csv(df_frequency, paste0(synthetic_data,"synthetic_attacker_modified.csv"), row.names = FALSE)
# Compare histogram ----
df_frequency <- read_csv(paste0(synthetic_data,"synthetic_attacker_modified.csv"))
df_graph_sds <- df_frequency %>%
filter(type == "synthetic")
df_graph_ods <- df_frequency %>%
filter(type == "original")
df_graph_ods <- unique(df_graph_ods)
df_graph <-
ggplot() +
geom_bar(data = df_graph_ods, aes(x = combine, y = Freq, fill = type), position = position_dodge(width=0.9), stat = "identity") +
geom_boxplot(position = position_dodge(width=0.9), aes(x = combine, y = Freq, fill = type), data = df_graph_sds) +
facet_wrap(~last_record, labeller = "label_both") +
scale_y_continuous(limits = c(0,100), breaks = seq(0,100,25)) +
theme_bw() +
theme(panel.grid.minor = element_blank(),
legend.position = "bottom",
legend.title = element_blank(),
legend.key.width=unit(1, "cm"),
axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5),
axis.title.x = element_blank(),
axis.line.y = element_line(color="black", linewidth=.5),
axis.line.x = element_line(color="black", linewidth=.5)
)
df_graph
ggsave(plot = df_graph, paste0(graphs,"graph_attacker_modified_mb.pdf"), height = 5, width = 10)
ggsave(plot = df_graph, paste0(graphs,"graph_attacker_modified_mb_v2.pdf"), height = 5, width = 5)
# Top commands ----
# Create empty R application (no figures, data frames, packages, etc.)
# Get a list of all loaded packages
packages <- search()[grepl("package:", search())]
# Unload each package
for (package in packages) {
unloadNamespace(package)
}
rm(list=ls(all=TRUE))
# load library
library(synthpop)
library(tidyverse)
library(car)
#functions
options(scipen=999)
# Load data ----
df_ods <- SD2011[, c("sex","edu","age","income","depress")]
# original ----
s5 <- syn(df_ods, seed = 8564, m = 1, print.flag = FALSE)
t5 <- disclosure( s5, df_ods, keys = c("sex","edu","age","income"), target = "depress", print.flag = FALSE)
# modified ----
table(df_ods$depress, useNA = "ifany")
df_ods$depress <- recode(df_ods$depress, "4:hi=2")
s6 <- syn(df_ods, seed = 8564, m = 1, print.flag = FALSE)
t6 <- disclosure( s6, df_ods, keys = c("sex","edu","age","income"), target = "depress", print.flag = FALSE)
# risk measure ----
print(t5, to.print = c("attrib"))
print(t6, to.print = c("attrib"))
# Identity disclosure measures ----
df_sds <- s5$syn
# Calculate DiSCO by hand
# Step 1: Create composite key q for GT and SD
df_ods$q <- paste(df_ods$sex, df_ods$edu, df_ods$agegr, df_ods$smoke, sep = "_")
df_sds$q <- paste(df_sds$sex, df_sds$edu, df_sds$agegr, df_sds$smoke, sep = "_")
# Step 2: Calculate iS
# iS : Proportion of all records in GT whose  q  value is found in SD.
iS <- 100 * mean(df_ods$q %in% df_sds$q)
iS
# Step 3: Calculate DiS
# DiS : Proportion of all records in GT where  q  in SD is disclosive (i.e.,  t  values for  q  are constant in SD).
DiS <- 100 * mean(sapply(1:nrow(df_ods), function(i) {
q <- df_ods$q[i]
sd_subset <- df_sds[df_sds$q == q, ]
length(unique(sd_subset$depress)) == 1  # Check if t values are constant
}))
DiS
# Assuming df_ods and df_sds are the ground truth and synthetic data frames
# Step 1: Identify disclosive keys in SD
disclosive_keys <- df_sds %>%
group_by(q) %>%                # Group by composite key q
summarize(disclosive = n_distinct(depress) == 1, .groups = "drop") # Check if t is constant
# Step 2: Merge disclosive status back to GT (df_ods)
df_ods_dis <- df_ods %>%
left_join(disclosive_keys, by = "q") %>%
mutate(disclosive = ifelse(is.na(disclosive), FALSE, disclosive)) # Handle missing keys
df_ods_dis_true <- df_ods_dis %>%
filter(disclosive==TRUE)
# Step 3: Calculate DiS
DiS <- 100 * mean(df_ods_dis$disclosive)
DiS
# Top commands ----
# Create empty R application (no figures, data frames, packages, etc.)
# Get a list of all loaded packages
packages <- search()[grepl("package:", search())]
# Unload each package
for (package in packages) {
unloadNamespace(package)
}
rm(list=ls(all=TRUE))
# load library
library(synthpop)
library(tidyverse)
#functions
options(scipen=999)
# Load data ----
df_ods <- SD2011[, c("sex", "age", "region","placesize","depress")]
# df_ods <- na.omit(df_ods)
df_ods[is.na(df_ods)] <- -8
# original ----
s5 <- syn(df_ods, seed = 8564, m = 1, print.flag = FALSE)
t5 <- disclosure( s5, df_ods, keys = c("sex", "age", "region", "placesize"), target = "depress", print.flag = FALSE)
df_sds <- s5$syn
t5
print(t5, to.print = c("ident"))
print(t5, to.print = c("attrib"))
# Identity disclosure measures ----
# Calculate DiSCO by hand
# Step 1: Create composite key q for GT and SD
df_ods$q <- paste(df_ods$sex, df_ods$age, df_ods$region, df_ods$placesize, sep = "_")
df_sds$q <- paste(df_sds$sex, df_sds$age, df_sds$region, df_sds$placesize, sep = "_")
# Step 2: Calculate iS
# iS : Proportion of all records in GT whose  q  value is found in SD.
iS <- 100 * mean(df_ods$q %in% df_sds$q)
iS
# Step 3: Calculate DiS
# DiS : Proportion of all records in GT where  q  in SD is disclosive (i.e.,  t  values for  q  are constant in SD).
DiS <- 100 * mean(sapply(1:nrow(df_ods), function(i) {
q <- df_ods$q[i]
sd_subset <- df_sds[df_sds$q == q, ]
length(unique(sd_subset$depress)) == 1  # Check if t values are constant
}))
DiS
