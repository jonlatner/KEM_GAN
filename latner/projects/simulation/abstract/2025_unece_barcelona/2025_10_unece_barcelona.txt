In this paper, we ask whether commonly used privacy measures accurately capture disclosure risk in synthetic data generated by CART-based models?  On the one hand, there is a general perception that generating synthetic data are easy, and to a certain degree this is true.  On the other hand, it is not always clear if the resulting synthetic data are in fact providing privacy protection. 

We evaluate standard and easily available privacy measures to estimate risk from synthetic data.  We use CART because previous research indicates that CART-based models generate synthetic data with both higher statistical utility and relatively lower privacy risks compared to other methods.  For privacy, we use two commonly understood measures of privacy: identity risk and attribute risk.  At the same time, findings are more generally applicable to both risk measures and synthetic data generators.

We use both simulated and real data. One is a simulated dataset a single observation we want to protect using synthetic data generated from a CART model.  We also use publicly available data from Social Diagnosis 2011 (SD2011) to extend our analysis into a real-world setting.

Our approach follows a structured framework. Using the simulated data and drawing on concepts from the attack model in differential privacy, we estimate risk measures for an attacker with full knowledge of the synthetic data generator, all synthetic observations, and all original observations except the disclosive record.  First, we test whether an attacker can identify the disclosive record.  They can.  Next, we test whether the privacy measures detect a known disclosure risk in the simulated data. They do not.  Finally, we extend the analysis using the SD2011 data to assess whether the measurement problem is specific to the simulated case or part of a broader challenge in detecting privacy risk. The results suggest the latter.

We make three contributions.  Common privacy metrics both may not capture disclosure risk in synthetic data generated from simulated data, and also may misstate privacy risks in real data.  Further, relatedly, and in contrast to previous research, CART-based models may produce synthetic data that sacrifices privacy protection for statistical utility.  Finally, we propose some solutions for measuring disclosure risk.  More generally, users interested in measuring privacy risk should be aware of the challenges we describe here.
